{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2D Animation Enabled - JAX CLIP Guided Diffusion v2.7 (huemin edit, Apr 2022)",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vcPH3L-MdwYS",
        "NcgnQE_sEjtz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelaza/DALL-E/blob/master/2D_Animation_Enabled_JAX_CLIP_Guided_Diffusion_v2_7_(huemin_edit%2C_Apr_2022).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZz-fFxNBCug"
      },
      "source": [
        "# Generates images from text prompts with CLIP guided diffusion.\n",
        "\n",
        "[huemin](https://twitter.com/huemin_art) edit based on [nshepperd's](https://twitter.com/nshepperd1) original notebook.\n",
        " - [nshepperd's JAX CLIP Guided Diffusion v2.7](https://colab.research.google.com/drive/1Z5kK1WXTkYoMAVN6FqkQg0Fa4bE5BnxG?usp=sharing)\n",
        "\n",
        "**Note the UI is minimal, use *Show code* or refer to google doc below for more info**\n",
        "\n",
        " - [Guide to Jax 2.7 - huemin edit (WIP)](https://docs.google.com/document/d/11HWN5e57taWdpyZlW5s6gqzrwMsLlmOQivyJncOPPhE/edit?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zKX4uWFBks2"
      },
      "source": [
        "#@title Changelog + MIT License{ display-mode: \"form\" }\n",
        "change_log = \"\"\"\n",
        "Change Log (last updated 2022.04.10)\n",
        " - made UI changes\n",
        " - added QOL stuff for google colab\n",
        " - added clip guided cc12m1 model from jax 2.6\n",
        " - added prompt weights\n",
        " - added batch prompts, random prompts, and random settings\n",
        " - added video outputs\n",
        " - added support for image prompts\n",
        " - added modified cuts\n",
        " - added init skip steps\n",
        " - added batch inits\n",
        " - added simple symmetry \n",
        " - added symmetry conds\n",
        " - added mean and variance conds\n",
        " - added database feature\n",
        " - added Prof RJ lerp models\n",
        " - animations\n",
        " - 2D animation key frames\n",
        " \"\"\"\n",
        "print(change_log)\n",
        "\n",
        "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
        "\n",
        "# Copyright (c) 2022 Katherine Crowson; nshepperd; huemin\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKZYWJt087dj"
      },
      "source": [
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLI2tEr0AUQ",
        "cellView": "form"
      },
      "source": [
        "#@markdown Mount to Google Drive\n",
        "googleDrive = True #@param {type:\"boolean\"}\n",
        "modelsOnDrive = True #@param {type:\"boolean\"}\n",
        "initImageFolder = True #@param {type:\"boolean\"}\n",
        "promptFolder = True #@param {type:\"boolean\"}\n",
        "\n",
        "outputFolder = \"AI\" #@param {type:\"string\"}\n",
        "v2 = \"nshepv2g/\"\n",
        "outputFolderStatic = outputFolder\n",
        "if outputFolderStatic == \"\":\n",
        "    outputFolderStatic = '/content/drive/MyDrive/AI/'\n",
        "else:\n",
        "    outputFolderStatic = '/content/drive/MyDrive/'+outputFolderStatic+'/'\n",
        "\n",
        "if googleDrive or modelsOnDrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Definitions"
      ],
      "metadata": {
        "id": "vcPH3L-MdwYS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ1HpnuQHOpU"
      },
      "source": [
        "import os\n",
        "if os.system(\"nvidia-smi | grep A100\") == 0:\n",
        "  !pip install -U https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.72+cuda111-cp37-none-manylinux2010_x86_64.whl \"jax==0.2.25\"\n",
        "else:\n",
        "  !pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.75%2Bcuda11.cudnn805-cp37-none-manylinux2010_x86_64.whl \"jax==0.3.0\"\n",
        "!pip install dm-haiku==0.0.5 cbor2 ftfy einops braceexpand\n",
        "!git clone https://github.com/nshepperd/CLIP_JAX\n",
        "!git clone https://github.com/nshepperd/jax-guided-diffusion -b v2.7\n",
        "!git clone https://github.com/crowsonkb/v-diffusion-jax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erek6UjoqR7O"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append('./CLIP_JAX')\n",
        "sys.path.append('./jax-guided-diffusion')\n",
        "sys.path.append('./v-diffusion-jax')\n",
        "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
        "\n",
        "from PIL import Image\n",
        "from braceexpand import braceexpand\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from subprocess import Popen, PIPE\n",
        "import functools\n",
        "import io\n",
        "import math\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.scipy as jsp\n",
        "import jaxtorch\n",
        "from jaxtorch import PRNG, Context, Module, nn, init\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython import display\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.transforms import functional as TF\n",
        "import torch.utils.data\n",
        "import torch\n",
        "\n",
        "from diffusion_models.common import DiffusionOutput, Partial, make_partial, blur_fft, norm1, LerpModels\n",
        "from diffusion_models.lazy import LazyParams\n",
        "from diffusion_models.schedules import cosine, ddpm, ddpm2, spliced\n",
        "from diffusion_models.perceptor import get_clip, clip_size, normalize\n",
        "\n",
        "from diffusion_models.aesthetic import AestheticLoss, AestheticExpected\n",
        "from diffusion_models.secondary import secondary1_wrap, secondary2_wrap\n",
        "from diffusion_models.antijpeg import anti_jpeg_cfg, jpeg_classifier, jpeg_classifier_wrap, jpeg_classifier_params\n",
        "from diffusion_models.pixelart import pixelartv4_wrap, pixelartv6_wrap\n",
        "from diffusion_models.pixelartv7 import pixelartv7_ic_attn\n",
        "from diffusion_models.cc12m_1 import cc12m_1_wrap, cc12m_1_cfg_wrap, cc12m_1_classifier_wrap\n",
        "from diffusion_models.openai import openai_256, openai_512, openai_512_finetune\n",
        "from diffusion_models.kat_models import danbooru_128, wikiart_128, wikiart_256, imagenet_128\n",
        "from diffusion_models import sampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTWBAsCCqtIz"
      },
      "source": [
        "devices = jax.devices()\n",
        "n_devices = len(devices)\n",
        "print('Using device:', devices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drive location for caching model parameters\n",
        "if modelsOnDrive:\n",
        "    model_location = '/content/drive/MyDrive/models'\n",
        "else:\n",
        "    model_location = '/content/models'\n",
        "\n",
        "os.makedirs(model_location, exist_ok=True)\n",
        "\n",
        "# Drive location for inits\n",
        "if initImageFolder:\n",
        "  init_location = outputFolderStatic+\"nshepv2g/\"+\"init/\"\n",
        "  os.makedirs(init_location, exist_ok=True)\n",
        "else:\n",
        "  init_location = ''\n",
        "\n",
        "# Drive location for prompt\n",
        "if promptFolder:\n",
        "  prompt_location = outputFolderStatic+\"nshepv2g/\"+\"prompts/\"\n",
        "  os.makedirs(prompt_location, exist_ok=True)\n",
        "\n",
        "# make video output folder path\n",
        "videoOutputFolder = outputFolder+\"videos/\""
      ],
      "metadata": {
        "id": "0H4hJlqlgg-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    local_path = os.path.join(model_location, basename)\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    else:\n",
        "        os.makedirs(f'{model_location}/tmp', exist_ok=True)\n",
        "        Popen(['curl', '--http1.1', url_or_path, '-o', f'{model_location}/tmp/{basename}']).wait()\n",
        "        os.rename(f'{model_location}/tmp/{basename}', local_path)\n",
        "        return local_path\n",
        "        \n",
        "LazyParams.fetch = fetch_model\n",
        "\n",
        "def grey(image):\n",
        "    [*_, c, h, w] = image.shape\n",
        "    return jnp.broadcast_to(image.mean(axis=-3, keepdims=True), image.shape)\n",
        "\n",
        "def cutout_image(image, offsetx, offsety, size, output_size=224):\n",
        "    \"\"\"Computes (square) cutouts of an image given x and y offsets and size.\"\"\"\n",
        "    (c, h, w) = image.shape\n",
        "\n",
        "    scale = jnp.stack([output_size / size, output_size / size])\n",
        "    translation = jnp.stack([-offsety * output_size / size, -offsetx * output_size / size])\n",
        "    return jax.image.scale_and_translate(image,\n",
        "                                         shape=(c, output_size, output_size),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def cutouts_images(image, offsetx, offsety, size, output_size=224):\n",
        "    f = partial(cutout_image, output_size=output_size)         # [c h w] [] [] [] -> [c h w]\n",
        "    f = jax.vmap(f, in_axes=(0, 0, 0, 0), out_axes=0)          # [n c h w] [n] [n] [n] -> [n c h w]\n",
        "    f = jax.vmap(f, in_axes=(None, 0, 0, 0), out_axes=0)       # [n c h w] [k n] [k n] [k n] -> [k n c h w]\n",
        "    return f(image, offsetx, offsety, size)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.0, p_grey=0.2, p_mixgrey=None, p_flip=0.5):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.p_grey = p_grey\n",
        "        self.p_mixgrey = p_mixgrey\n",
        "        self.p_flip = p_flip\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "\n",
        "        small_cuts = self.cutn//2\n",
        "        large_cuts = self.cutn - self.cutn//2\n",
        "\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[small_cuts, n])**self.cut_pow\n",
        "        sizes = (min_size + cut_us * (max_size - min_size)).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=w - sizes)\n",
        "        offsets_y = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=h - sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "\n",
        "        B1 = 40\n",
        "        B2 = 40\n",
        "        lcut_us = jax.random.uniform(rng.split(), shape=[large_cuts, n])\n",
        "        border = B1 + lcut_us * B2\n",
        "        lsizes = (max(h,w) + border).astype(jnp.int32)\n",
        "        loffsets_x = jax.random.uniform(rng.split(), [large_cuts, n], minval=w/2-lsizes/2-border, maxval=w/2-lsizes/2+border)\n",
        "        loffsets_y = jax.random.uniform(rng.split(), [large_cuts, n], minval=h/2-lsizes/2-border, maxval=h/2-lsizes/2+border)\n",
        "        lcutouts = cutouts_images(input, loffsets_x, loffsets_y, lsizes)\n",
        "\n",
        "        cutouts = jnp.concatenate([cutouts, lcutouts], axis=0)\n",
        "\n",
        "        greyed = grey(cutouts)\n",
        "\n",
        "        if self.p_mixgrey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          grey_rs = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_mixgrey, grey_rs * greyed + (1 - grey_rs) * cutouts, cutouts)\n",
        "\n",
        "        if self.p_grey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_grey, greyed, cutouts)\n",
        "\n",
        "        if self.p_flip is not None:\n",
        "          flip_us = jax.random.bernoulli(rng.split(), self.p_flip, [self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(flip_us, jnp.flip(cutouts, axis=-1), cutouts)\n",
        "\n",
        "        return cutouts\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.cut_pow, self.p_grey, self.p_mixgrey, self.p_flip], (self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        return MakeCutouts(cut_size, cutn, *dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutouts_huemin(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.0, p_grey=0.2, p_mixgrey=None, p_flip=0.5):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.p_grey = p_grey\n",
        "        self.p_mixgrey = p_mixgrey\n",
        "        self.p_flip = p_flip\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "\n",
        "        small_cuts = self.cutn//2\n",
        "        large_cuts = self.cutn - self.cutn//2\n",
        "\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "\n",
        "        cut_power = np.random.gamma(1, 1, 1)[0]*self.cut_pow\n",
        "\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[small_cuts, n])**cut_power\n",
        "        sizes = (min_size + cut_us * (max_size - min_size)).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=w - sizes)\n",
        "        offsets_y = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=h - sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "\n",
        "        B1 = np.random.gamma(1, max_size/4, 1)[0]\n",
        "        B2 = np.random.gamma(1, max_size/4, 1)[0]\n",
        "        \n",
        "        lcut_us = jax.random.uniform(rng.split(), shape=[large_cuts, n])\n",
        "        border = B1 + lcut_us * B2\n",
        "        lsizes = (max(h,w) + border).astype(jnp.int32)\n",
        "        loffsets_x = jax.random.uniform(rng.split(), [large_cuts, n], minval=w/2-lsizes/2-border, maxval=w/2-lsizes/2+border)\n",
        "        loffsets_y = jax.random.uniform(rng.split(), [large_cuts, n], minval=h/2-lsizes/2-border, maxval=h/2-lsizes/2+border)\n",
        "        lcutouts = cutouts_images(input, loffsets_x, loffsets_y, lsizes)\n",
        "\n",
        "        cutouts = jnp.concatenate([cutouts, lcutouts], axis=0)\n",
        "\n",
        "        greyed = grey(cutouts)\n",
        "\n",
        "        if self.p_mixgrey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          grey_rs = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_mixgrey, grey_rs * greyed + (1 - grey_rs) * cutouts, cutouts)\n",
        "\n",
        "        if self.p_grey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_grey, greyed, cutouts)\n",
        "\n",
        "        if self.p_flip is not None:\n",
        "          flip_us = jax.random.bernoulli(rng.split(), self.p_flip, [self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(flip_us, jnp.flip(cutouts, axis=-1), cutouts)\n",
        "\n",
        "        return cutouts\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.cut_pow, self.p_grey, self.p_mixgrey, self.p_flip], (self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        return MakeCutouts_huemin(cut_size, cutn, *dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutoutsPixelated(object):\n",
        "    def __init__(self, make_cutouts, factor=4):\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.factor = factor\n",
        "        self.cutn = make_cutouts.cutn\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        input = jax.image.resize(input, [n, c, h*self.factor, w * self.factor], method='nearest')\n",
        "        return self.make_cutouts(input, key)\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.make_cutouts], [self.factor])\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MakeCutoutsPixelated(*dynamic, *static)\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = norm1(x)\n",
        "    y = norm1(y)\n",
        "    return (x - y).square().sum(axis=-1).sqrt().div(2).arcsin().square().mul(2)\n"
      ],
      "metadata": {
        "id": "wVrjUvv0N-9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsLm7ElGc6nQ"
      },
      "source": [
        "# Define combinators.\n",
        "\n",
        "# These (ab)use the jax pytree registration system to define parameterised\n",
        "# objects for doing various things, which are compatible with jax.jit.\n",
        "\n",
        "# For jit compatibility an object needs to act as a pytree, which means implementing two methods:\n",
        "#  - tree_flatten(self): returns two lists of the object's fields:\n",
        "#       1. 'dynamic' parameters: things which can be jax tensors, or other pytrees\n",
        "#       2. 'static' parameters: arbitrary python objects, will trigger recompilation when changed\n",
        "#  - tree_unflatten(static, dynamic): reconstitutes the object from its parts\n",
        "\n",
        "# With these tricks, you can simply define your cond_fn as an object, as is done\n",
        "# below, and pass it into the jitted sample step as a regular argument. JAX will\n",
        "# handle recompiling the jitted code whenever a control-flow affecting parameter\n",
        "# is changed (such as cut_batches).\n",
        "\n",
        "# A wrapper that causes the diffusion model to generate tileable images, by\n",
        "# randomly shifting the image with wrap around.\n",
        "\n",
        "def xyroll(x, shifts):\n",
        "  return jax.vmap(partial(jnp.roll, axis=[1,2]), in_axes=(0, 0))(x, shifts)\n",
        "\n",
        "@make_partial\n",
        "def TilingModel(model, x, cosine_t, key):\n",
        "  rng = PRNG(key)\n",
        "  [n, c, h, w] = x.shape\n",
        "  shift = jax.random.randint(rng.split(), [n, 2], -50, 50)\n",
        "  x = xyroll(x, shift)\n",
        "  out = model(x, cosine_t, rng.split())\n",
        "  def unshift(val):\n",
        "    return xyroll(val, -shift)\n",
        "  return jax.tree_util.tree_map(unshift, out)\n",
        "\n",
        "@make_partial\n",
        "def PanoramaModel(model, x, cosine_t, key):\n",
        "  rng = PRNG(key)\n",
        "  [n, c, h, w] = x.shape\n",
        "  shift = jax.random.randint(rng.split(), [n, 2], 0, [1, w])\n",
        "  x = xyroll(x, shift)\n",
        "  out = model(x, cosine_t, rng.split())\n",
        "  def unshift(val):\n",
        "    return xyroll(val, -shift)\n",
        "  return jax.tree_util.tree_map(unshift, out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models & Parameters"
      ],
      "metadata": {
        "id": "xbkukxq6d3Qh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlN36wo_TaFC"
      },
      "source": [
        "# Pixel art model\n",
        "# There are many checkpoints supported with this model, so maybe better to provide choice in the notebook\n",
        "pixelartv4_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_34.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_63.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_150.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_50.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_65.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_97.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_173.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_344.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_432.pt'\n",
        "    'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_600.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_700.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_800.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_1000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_2000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_3000.pt'\n",
        "    , key='params_ema'\n",
        ")\n",
        "\n",
        "pixelartv6_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-1000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-2000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-3000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-4000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-aug-900.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-aug-1300.pt'\n",
        "    'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-aug-3000.pt'\n",
        "    , key='params_ema'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cc12m_1\n",
        "cc12m_1_params = LazyParams.pt('https://the-eye.eu/public/AI/models/v-diffusion/cc12m_1.pth')"
      ],
      "metadata": {
        "id": "NJ0jd8CasYzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQ2Di_LRM46"
      },
      "source": [
        "# Losses and cond fn.\n",
        "\n",
        "def filternone(xs):\n",
        "  return [x for x in xs if x is not None]\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondCLIP(object):\n",
        "    \"\"\"Backward a loss function through clip.\"\"\"\n",
        "    def __init__(self, perceptor, make_cutouts, cut_batches, *losses):\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.cut_batches = cut_batches\n",
        "        self.losses = filternone(losses)\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts).rearrange('(k n) c -> k n c', n=n)\n",
        "            return sum(loss_fn(image_embeds) for loss_fn in self.losses)\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.perceptor, self.make_cutouts, self.losses], [self.cut_batches]\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        [perceptor, make_cutouts, losses] = dynamic\n",
        "        [cut_batches] = static\n",
        "        return cls(perceptor, make_cutouts, cut_batches, *losses)\n",
        "\n",
        "@make_partial\n",
        "def SphericalDistLoss(text_embed, clip_guidance_scale, image_embeds):\n",
        "    losses = spherical_dist_loss(image_embeds, text_embed).mean(0)\n",
        "    return (clip_guidance_scale * losses).sum()\n",
        "\n",
        "@make_partial\n",
        "def InfoLOOB(text_embed, clip_guidance_scale, inv_tau, lm, image_embeds):\n",
        "    all_image_embeds = norm1(image_embeds.mean(0))\n",
        "    all_text_embeds = norm1(text_embed)\n",
        "    sim_matrix = inv_tau * jnp.einsum('nc,mc->nm', all_image_embeds, all_text_embeds)\n",
        "    xn = sim_matrix.shape[0]\n",
        "    def loob(sim_matrix):\n",
        "      diag = jnp.eye(xn) * sim_matrix\n",
        "      off_diag = (1 - jnp.eye(xn))*sim_matrix + jnp.eye(xn) * float('-inf')\n",
        "      return -diag.sum() + lm * jsp.special.logsumexp(off_diag, axis=-1).sum()\n",
        "    losses = loob(sim_matrix) + loob(sim_matrix.transpose())\n",
        "    return losses.sum() * clip_guidance_scale.mean() / inv_tau\n",
        "\n",
        "@make_partial\n",
        "def CondTV(tv_scale, x_in, key):\n",
        "    def downscale2d(image, f):\n",
        "        [c, n, h, w] = image.shape\n",
        "        return jax.image.resize(image, [c, n, h//f, w//f], method='cubic')\n",
        "\n",
        "    def tv_loss(input):\n",
        "        \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "        x_diff = input[..., :, 1:] - input[..., :, :-1]\n",
        "        y_diff = input[..., 1:, :] - input[..., :-1, :]\n",
        "        return x_diff.square().mean([1,2,3]) + y_diff.square().mean([1,2,3])\n",
        "\n",
        "    def sum_tv_loss(x_in, f=None):\n",
        "        if f is not None:\n",
        "            x_in = downscale2d(x_in, f)\n",
        "        return tv_loss(x_in).sum() * tv_scale\n",
        "    tv_grad_512 = jax.grad(sum_tv_loss)(x_in)\n",
        "    tv_grad_256 = jax.grad(partial(sum_tv_loss,f=2))(x_in)\n",
        "    tv_grad_128 = jax.grad(partial(sum_tv_loss,f=4))(x_in)\n",
        "    return tv_grad_512 + tv_grad_256 + tv_grad_128\n",
        "\n",
        "@make_partial\n",
        "def CondRange(range_scale, x_in, key):\n",
        "    def range_loss(x_in):\n",
        "        return jnp.abs(x_in - x_in.clamp(minval=-1,maxval=1)).mean()\n",
        "    return range_scale * jax.grad(range_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondHorizontalSymmetry(horizontal_symmetry_scale, x_in, key):\n",
        "    def horizontal_symmetry_loss(x_in):\n",
        "        [n, c, h, w] = x_in.shape\n",
        "        return jnp.abs(x_in[:, :, :, :w//2]-jnp.flip(x_in[:, :, :, w//2:],-1)).mean()\n",
        "    return horizontal_symmetry_scale * jax.grad(horizontal_symmetry_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondVerticalSymmetry(vertical_symmetry_scale, x_in, key):\n",
        "    def vertical_symmetry_loss(x_in):\n",
        "        [n, c, h, w] = x_in.shape\n",
        "        return jnp.abs(x_in[:, :, :h//2, :]-jnp.flip(x_in[:, :, h//2:, :],-2)).mean()\n",
        "    return vertical_symmetry_scale * jax.grad(vertical_symmetry_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMean(mean_scale, x_in, key):\n",
        "    def mean_loss(x_in):\n",
        "        return jnp.abs(x_in-0.5).mean()\n",
        "    return mean_scale * jax.grad(mean_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondVar(var_scale, x_in, key):\n",
        "    def var_loss(x_in):\n",
        "        return x_in.var()\n",
        "    return var_scale * jax.grad(var_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMSE(target, mse_scale, x_in, key):\n",
        "    def mse_loss(x_in):\n",
        "        return (x_in - target).square().mean()\n",
        "    return mse_scale * jax.grad(mse_loss)(x_in)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MaskedMSE(object):\n",
        "    # MSE loss. Targets the output towards an image.\n",
        "    def __init__(self, target, mse_scale, mask, grey=False):\n",
        "        self.target = target\n",
        "        self.mse_scale = mse_scale\n",
        "        self.mask = mask\n",
        "        self.grey = grey\n",
        "    def __call__(self, x_in, key):\n",
        "        def mse_loss(x_in):\n",
        "            if self.grey:\n",
        "              return (self.mask * grey(x_in - self.target).square()).mean()\n",
        "            else:\n",
        "              return (self.mask * (x_in - self.target).square()).mean()\n",
        "        return self.mse_scale * jax.grad(mse_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.target, self.mse_scale, self.mask], [self.grey]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MaskedMSE(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MainCondFn(object):\n",
        "    # Used to construct the main cond_fn. Accepts a diffusion model which will\n",
        "    # be used for denoising, plus a list of 'conditions' which will\n",
        "    # generate gradient of a loss wrt the denoised, to be summed together.\n",
        "    def __init__(self, diffusion, conditions, blur_amount=None, use='pred'):\n",
        "        self.diffusion = diffusion\n",
        "        self.conditions = [c for c in conditions if c is not None]\n",
        "        self.blur_amount = blur_amount\n",
        "        self.use = use\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, x, cosine_t, key):\n",
        "        if not self.conditions:\n",
        "          return jnp.zeros_like(x)\n",
        "\n",
        "        rng = PRNG(key)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        alphas, sigmas = cosine.to_alpha_sigma(cosine_t)\n",
        "\n",
        "        def denoise(key, x):\n",
        "            pred = self.diffusion(x, cosine_t, key).pred\n",
        "            if self.use == 'pred':\n",
        "                return pred\n",
        "            elif self.use == 'x_in':\n",
        "                return pred * sigmas + x * alphas\n",
        "        (x_in, backward) = jax.vjp(partial(denoise, rng.split()), x)\n",
        "\n",
        "        total = jnp.zeros_like(x_in)\n",
        "        for cond in self.conditions:\n",
        "            total += cond(x_in, rng.split())\n",
        "        if self.blur_amount is not None:\n",
        "          blur_radius = (self.blur_amount * sigmas / alphas).clamp(0.05,512)\n",
        "          total = blur_fft(total, blur_radius.mean())\n",
        "        final_grad = -backward(total)[0]\n",
        "\n",
        "        # clamp gradients to a max of 0.2\n",
        "        magnitude = final_grad.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "        final_grad = final_grad * jnp.where(magnitude > 0.2, 0.2 / magnitude, 1.0)\n",
        "        return final_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.diffusion, self.conditions, self.blur_amount], [self.use]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MainCondFn(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondFns(object):\n",
        "    def __init__(self, *conditions):\n",
        "        self.conditions = conditions\n",
        "    def __call__(self, x, t, key):\n",
        "        rng = PRNG(key)\n",
        "        total = jnp.zeros_like(x)\n",
        "        for cond in self.conditions:\n",
        "          total += cond(x, t, key)\n",
        "        return total\n",
        "    def tree_flatten(self):\n",
        "        return [self.conditions], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [conditions] = dynamic\n",
        "        return CondFns(*conditions)\n",
        "\n",
        "def clamp_score(score):\n",
        "  magnitude = score.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "  return score * jnp.where(magnitude > 0.1, 0.1 / magnitude, 1.0)\n",
        "\n",
        "@make_partial\n",
        "def BlurRangeLoss(scale, x, cosine_t, key):\n",
        "    def blurred_pred(x, cosine_t):\n",
        "      alpha, sigma = cosine.to_alpha_sigma(cosine_t)\n",
        "      blur_radius = (sigma / alpha * 2)\n",
        "      return blur_fft(x, blur_radius) / alpha.clamp(0.01)\n",
        "    def loss(x):\n",
        "        pred = blurred_pred(x, cosine_t)\n",
        "        diff = pred - pred.clamp(minval=-1,maxval=1)\n",
        "        return diff.square().sum()\n",
        "    return clamp_score(-scale * jax.grad(loss)(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_prompt(clip,all_prompt):\n",
        "  embeds = []\n",
        "  expands = all_prompt.split(\"|\")\n",
        "  for prompt in expands:\n",
        "    prompt = prompt.strip()\n",
        "    # check url\n",
        "    if \"https:\" in prompt:\n",
        "      tmp = prompt.split(\":\")\n",
        "      # check weight\n",
        "      if len(tmp) == 2:\n",
        "        temp_weight = 1\n",
        "        temp_prompt = prompt\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here1\")\n",
        "        #print(tmp_embed.shape)\n",
        "      if len(tmp) == 3:\n",
        "        temp_prompt = \":\".join(tmp[0:2]).strip()\n",
        "        temp_weight = float(tmp[2].strip())\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here2\")\n",
        "        #print(tmp_embed.shape)\n",
        "    # if not url\n",
        "    else:\n",
        "      # check weight\n",
        "      if ':' in prompt:\n",
        "        tmp = prompt.split(\":\")\n",
        "        temp_prompt = tmp[0].strip()\n",
        "        temp_weight = float(tmp[1].strip())\n",
        "      else:\n",
        "        temp_prompt = prompt\n",
        "        temp_weight = 1\n",
        "      # try path\n",
        "      try:\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "      except:\n",
        "        tmp_embed = temp_weight * clip.embed_text(temp_prompt.strip())\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here4\")\n",
        "        #print(tmp_embed.shape)\n",
        "  return norm1(sum(embeds))\n",
        "\n",
        "def process_prompts(clip, prompts):\n",
        "  return jnp.stack([process_prompt(clip, prompt) for prompt in prompts])\n",
        "\n",
        "def expand(xs, batch_size):\n",
        "  \"\"\"Extend or truncate the list of prompts to the batch size.\"\"\"\n",
        "  return (xs * batch_size)[:batch_size]"
      ],
      "metadata": {
        "id": "7t4coj2v4mQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_folder(outputFolder, choose_diffusion_model, batch_outputFolder, use_batch_outputFolder):\n",
        "    if googleDrive:\n",
        "        yearMonth = time.strftime('/%Y-%m/')\n",
        "        outputFolder = outputFolderStatic+v2+choose_diffusion_model+yearMonth\n",
        "        if use_batch_outputFolder and not batch_outputFolder == \"\":\n",
        "            outputFolder += batch_outputFolder+\"/\"\n",
        "        os.makedirs(outputFolder, exist_ok=True)\n",
        "    return outputFolder\n",
        "\n",
        "def save_still_settings(local_seed,path,tag):\n",
        "  setting_list = {\n",
        "      'seed': local_seed,\n",
        "      'image_size' : image_size,\n",
        "      'batch_size' : batch_size,\n",
        "      'n_batches' : n_batches,\n",
        "      'steps' : steps,\n",
        "\n",
        "      'choose_diffusion_model' : choose_diffusion_model,\n",
        "      'use_secondary_model' : use_secondary_model,\n",
        "      'use_anti_jpeg' : use_antijpeg,\n",
        "      'clips' : clips,\n",
        "\n",
        "      'cutn' : cutn,\n",
        "      'cut_batches' : cut_batches,\n",
        "      'cut_pow' : cut_pow,\n",
        "      'cut_p_mixgrey' : cut_p_mixgrey,\n",
        "      'cut_p_grey' : cut_p_grey,\n",
        "      'cut_p_flip' : cut_p_flip,\n",
        "\n",
        "      'sample_mode' : sample_mode,\n",
        "      'eta' : eta,\n",
        "      'starting_noise' : starting_noise,\n",
        "      'ending_noise' : ending_noise,\n",
        "      'skip_percent' : skip_percent,\n",
        "\n",
        "      'ic_cond' : ic_cond,\n",
        "      'ic_guidance_scale' : ic_guidance_scale,\n",
        "      'cfg_guidance_scale' : cfg_guidance_scale,\n",
        "      'aesthetic_loss_scale' : aesthetic_loss_scale,\n",
        "      'clip_guidance_scale' : clip_guidance_scale,\n",
        "      'antijpeg_guidance_scale' : antijpeg_guidance_scale,\n",
        "      'tv_scale' : tv_scale,\n",
        "      'range_scale' : range_scale,\n",
        "      'mean_scale' : mean_scale,\n",
        "      'var_scale' : var_scale,\n",
        "      'horizontal_symmetry_scale' : horizontal_symmetry_scale,\n",
        "      'vertical_symmetry_scale' : vertical_symmetry_scale,\n",
        "\n",
        "      'use_vertical_symmetry' : use_vertical_symmetry,\n",
        "      'use_horizontal_symmetry' : use_horizontal_symmetry,\n",
        "      'transformation_schedule' : transformation_schedule,\n",
        "\n",
        "      'use_init' : use_init,\n",
        "      'init_image' : init_image,\n",
        "      'init_weight_mse' : init_weight_mse,\n",
        "\n",
        "      'max_frames' : max_frames,\n",
        "      'prev_frame_clip_guidance_scale' : prev_frame_clip_guidance_scale,\n",
        "      'prev_frame_cfg_guidance_scale' : prev_frame_cfg_guidance_scale,\n",
        "      'prev_frame_starting_noise' : prev_frame_starting_noise,\n",
        "      'prev_frame_skip_percent' : prev_frame_skip_percent,\n",
        "      'prev_frame_weight_mse' : prev_frame_weight_mse,\n",
        "      'use_prev_frame_image_prompt' : use_prev_frame_image_prompt,\n",
        "\n",
        "      'key_frames' : key_frames,\n",
        "      'max_frames' : max_frames,\n",
        "      'angle' : angle,\n",
        "      'zoom' : zoom,\n",
        "      'translation_x' : translation_x,\n",
        "      'translation_y' : translation_y,\n",
        "\n",
        "      'all_title' : title\n",
        "\n",
        "      }\n",
        "  with open(f\"{path}{tag}.txt\", \"w+\") as f:\n",
        "    json.dump(setting_list, f, ensure_ascii=False, indent=4)\n",
        "  return\n",
        "\n",
        "def simple_symmetry(x_in):\n",
        "  [n, c, h, w] = x_in.shape\n",
        "  x_in = jnp.concatenate([x_in[:, :, :, :w//2], jnp.flip(x_in[:, :, :, :w//2],-1)], -1)\n",
        "  return(x_in)\n",
        "\n",
        "def load_image(url):\n",
        "    init_array = Image.open(fetch(url)).convert('RGB')\n",
        "    init_array = init_array.resize(image_size, Image.LANCZOS)\n",
        "    init_array = jnp.array(TF.to_tensor(init_array)).unsqueeze(0).mul(2).sub(1)\n",
        "    return init_array\n",
        "\n",
        "def display_images(images):\n",
        "  images = images.add(1).div(2).clamp(0, 1)\n",
        "  images = torch.tensor(np.array(images))\n",
        "  grid = utils.make_grid(images, 4).cpu()\n",
        "  display.display(TF.to_pil_image(grid))\n",
        "  return\n",
        "\n",
        "if promptFolder:\n",
        "  # makes template csv files if prompt_location is empty\n",
        "  if len(os.listdir(prompt_location)) == 0:\n",
        "    subjects_df = pd.DataFrame({\"subject\" : [\"rifle\",\"sword\"]})\n",
        "    modifiers_df = pd.DataFrame({\"modifier\" : [\"cosmic\",\"void\"]})\n",
        "    artists_df = pd.DataFrame({\"artist\" : [\"steven belledin\",\"dan mumford\"]})\n",
        "    subjects_df.to_csv(prompt_location+\"subjects.csv\",index=False)\n",
        "    modifiers_df.to_csv(prompt_location+\"modifiers.csv\",index=False)\n",
        "    artists_df.to_csv(prompt_location+\"artists.csv\",index=False)\n",
        "    print(\"creating random prompt csv files\")\n",
        "  else:\n",
        "    print(f\"{len(os.listdir(prompt_location))} files in {prompt_location}\")\n",
        "\n",
        "def filternone(xs):\n",
        "  return [x for x in xs if x is not None]\n",
        "\n",
        "class LerpWeightError(Exception):\n",
        "       pass"
      ],
      "metadata": {
        "id": "cgEXTESXiJW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prof. R.J. Lerp Models"
      ],
      "metadata": {
        "id": "NcgnQE_sEjtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Lerp Settings\n",
        "# Combines the outputs of different models, used if LerpedModels is chosen as the diffusion model.\n",
        "# The `cond_model` is a secondary model used to help diffuse, `secondary2` is best for speed.\n",
        "choose_cond_model = \"secondary2\" #@param [\"secondary2\", \"OpenAI256\", \"PixelArtv6\", \"PixelArtv7\", \"PixelArtv4\", \"cc12m\", \"cc12m_cfg\", \"WikiArt\", \"Danbooru\", \"Imagenet128\"] \n",
        "lerpWeights = []\n",
        "\n",
        "#---\n",
        "#The total sum of weights must add up to 1.0.\n",
        "###### `use_antijpeg` will include the antijpeg model in the lerp, resulting in clearer results. `use_MakeCutoutsPixelated` will use the cutout method meant for the pixelart models.\n",
        "use_MakeCutoutsPixelated = False #@param {type:\"boolean\"}\n",
        "\n",
        "OpenAI512_weight = 0 #@param {type:\"number\"}\n",
        "if OpenAI512_weight != 0:\n",
        "    lerpWeights.append(OpenAI512_weight)\n",
        "\n",
        "OpenAI256_weight = 0 #@param {type:\"number\"}\n",
        "if OpenAI256_weight != 0:\n",
        "    lerpWeights.append(OpenAI256_weight)\n",
        "\n",
        "OpenAIFinetune_weight = 0.3 #@param {type:\"number\"}\n",
        "if OpenAIFinetune_weight != 0:\n",
        "    lerpWeights.append(OpenAIFinetune_weight)\n",
        "\n",
        "PixelArtv4_weight = 0 #@param {type:\"number\"}\n",
        "if PixelArtv4_weight != 0:\n",
        "    lerpWeights.append(PixelArtv4_weight)\n",
        "\n",
        "PixelArtv6_weight = 0 #@param {type:\"number\"}\n",
        "if PixelArtv6_weight != 0:\n",
        "    lerpWeights.append(PixelArtv6_weight)\n",
        "\n",
        "PixelArtv7_weight =  0#@param {type:\"number\"}\n",
        "if PixelArtv7_weight != 0:\n",
        "    lerpWeights.append(PixelArtv7_weight)\n",
        "\n",
        "cc12m_weight = 0 #@param {type:\"number\"}\n",
        "if cc12m_weight != 0:\n",
        "    lerpWeights.append(cc12m_weight)\n",
        "\n",
        "cc12m_cfg_weight = 0 #@param {type:\"number\"}\n",
        "if cc12m_cfg_weight != 0:\n",
        "    lerpWeights.append(cc12m_cfg_weight)\n",
        "\n",
        "WikiArt_weight = 0.7 #@param {type:\"number\"}\n",
        "if WikiArt_weight != 0:\n",
        "    lerpWeights.append(WikiArt_weight)\n",
        "\n",
        "Danbooru_weight = 0 #@param {type:\"number\"}\n",
        "if Danbooru_weight != 0:\n",
        "    lerpWeights.append(Danbooru_weight)\n",
        "\n",
        "Imagenet128_weight = 0 #@param {type:\"number\"}\n",
        "if Imagenet128_weight != 0:\n",
        "    lerpWeights.append(Imagenet128_weight)\n",
        "\n",
        "secondary2_weight = 0 #@param {type:\"number\"}\n",
        "if secondary2_weight != 0:\n",
        "    lerpWeights.append(secondary2_weight)\n",
        "\n",
        "totalWeight = sum(lerpWeights)\n",
        "if totalWeight != 1.0:\n",
        "    raise LerpWeightError(\"Total weights must add up to 1.0.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RbXg7XodEkjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "d-nGPkHFeS0Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGgJmRzq3Cs",
        "cellView": "form"
      },
      "source": [
        "#@markdown Output Settings\n",
        "use_batch_outputFolder = True #@param {type:\"boolean\"}\n",
        "batch_outputFolder = \"animation_test\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Run Settings\n",
        "seed = None #@param {type:\"raw\"} # if None, uses the current time in seconds.\n",
        "image_size = (512,512) #@param {type:\"raw\"}\n",
        "batch_size = 1 #@param {type:\"integer\"}\n",
        "n_batches = 1 #@param {type:\"integer\"}\n",
        "steps = 50     #@param {type:\"raw\"} # Number of steps for sampling. Generally, more = better.\n",
        "\n",
        "#@markdown Diffusion and CLIP Settings\n",
        "choose_diffusion_model = \"cc12m\" #@param [\"LerpedModels\",\"OpenAI\", \"OpenAIFinetune\", \"OpenAI256\", \"cc12m_cfg\", \"cc12m\", \"PixelArtv4\", \"WikiArt\", \"PixelArtv7_ic_attn\", \"PixelArtv6\",\"Danbooru\", \"Imagenet\"]\n",
        "use_secondary_model = False #@param {type:\"boolean\"}\n",
        "use_antijpeg = False #@param {type:\"boolean\"}\n",
        "use_vitb16 = True #@param {type:\"boolean\"}\n",
        "use_vitb32 = False #@param {type:\"boolean\"}\n",
        "use_vitl14 = False #@param {type:\"boolean\"}\n",
        "clips = ['ViT-B/16' if use_vitb16 else None, 'ViT-B/32' if use_vitb32 else None, 'ViT-L/14' if use_vitl14 else None]\n",
        "clips = filternone(clips)\n",
        "\n",
        "#@markdown Cut Settings\n",
        "cutn =         16#@param {type:\"raw\"} # Effective cutn is cut_batches * this\n",
        "cut_batches = 2 #@param {type:\"raw\"} \n",
        "cut_pow = 1.0   #@param {type:\"raw\"} # Affects the size of cutouts. Larger cut_pow -> smaller cutouts (down to the min of 224x244)\n",
        "cut_p_mixgrey = None #@param {type:\"raw\"} # Partially greyscale some cuts. Has weird effect.\n",
        "cut_p_grey = 0.2     #@param {type:\"raw\"} # Fully greyscale some cuts. Tends to improve coherence.\n",
        "cut_p_flip = 0.5     #@param {type:\"raw\"} # Flip 50% of cuts to make clip effectively horizontally equivariant. Improves coherence.\n",
        "use_huemin_cuts = False #test@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Noise Settings\n",
        "# sample_mode:\n",
        "#  prk : high quality, 3x slow (eta=0)\n",
        "#  plms : high quality, about as fast as ddim (eta=0)\n",
        "#  ddim : traditional, accepts eta for different noise levels which sometimes have nice aesthetic effect\n",
        "sample_mode = 'ddim' #@param [\"ddim\", \"plms\", \"prk\"]\n",
        "eta = 0.8       #@param {type:\"raw\"} # Only applies to ddim sample loop: 0.0: DDIM | 1.0: DDPM | -1.0: Extreme noise (q_sample)\n",
        "starting_noise = 1.0   #@param {type:\"raw\"} # Between 0 and 1. When using init image, generally 0.5-0.8 is good. Lower starting noise makes the result look more like the init.\n",
        "ending_noise = 0.0     #@param {type:\"raw\"} # Usually 0.0 for high detail. Can set a little higher like 0.05 to end early for smoother looking result.\n",
        "skip_percent = 0.0   #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown Cond Settings\n",
        "ic_cond = \"None\" #@param {type:\"string\"}\n",
        "ic_guidance_scale = 0 #@param {type:\"raw\"} # For pixelartv7_ic_attn\n",
        "cfg_guidance_scale =  0#@param {type:\"raw\"} # For cc12m_1_cfg\n",
        "aesthetic_loss_scale = 0.0 #@param {type:\"raw\"} # For aesthetic loss, requires ViT-B/16\n",
        "clip_guidance_scale = 80000 #@param {type:\"raw\"} # Note: with two perceptors, effective guidance scale is ~2x because they are added together.\n",
        "antijpeg_guidance_scale =  0 #@param {type:\"raw\"}\n",
        "tv_scale = 0  #@param {type:\"raw\"} # Smooths out the image\n",
        "range_scale =  0#@param {type:\"raw\"} # Tries to prevent pixel values from going out of range\n",
        "mean_scale =  0#@param {type:\"raw\"} # trends towards middle grey\n",
        "var_scale =  0#@param {type:\"raw\"} # lowers image variation\n",
        "horizontal_symmetry_scale =  0#@param {type:\"raw\"}\n",
        "vertical_symmetry_scale =  0#@param {type:\"raw\"}\n",
        "\n",
        "#@markdown Transformation Settings\n",
        "use_vertical_symmetry = False #@param {type:\"boolean\"}\n",
        "use_horizontal_symmetry = False #@param {type:\"boolean\"}\n",
        "transformation_schedule = \"0.1\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Init Settings\n",
        "use_init = False   #@param {type:\"boolean\"}\n",
        "init_image = \"\"      #@param {type:\"string\"} \n",
        "# Diffusion will start with a mixture of this image with noise.\n",
        "init_weight_mse = 0    #@param {type:\"raw\"} # MSE loss between the output and the init makes the result look more like the init (should be between 0 and width*height*3).\n",
        "\n",
        "#@markdown Animation Settings\n",
        "max_frames = 1000 #@param {type:\"number\"}\n",
        "prev_frame_clip_guidance_scale = 160000 #@param {type:\"raw\"} # Note: with two perceptors, effective guidance scale is ~2x because they are added together.\n",
        "prev_frame_cfg_guidance_scale = 0 #@param {type:\"raw\"}\n",
        "prev_frame_starting_noise = 1.0 #@param{type: 'number'}\n",
        "prev_frame_ending_noise = 0.0 #@param{type: 'number'}\n",
        "prev_frame_skip_percent = 0.5 #@param{type: 'number'}\n",
        "prev_frame_weight_mse = 100 #@param{type: 'number'}\n",
        "\n",
        "use_prev_frame_image_prompt = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Keyframe Settings\n",
        "key_frames = True #@param {type:\"boolean\"}\n",
        "max_frames = 1000#@param {type:\"number\"}\n",
        "angle = \"0:(0.5)\"#@param {type:\"string\"}\n",
        "zoom = \"0:(1.05)\"#@param {type:\"string\"}\n",
        "translation_x = \"0: (0.0)\"#@param {type:\"string\"}\n",
        "translation_y = \"0: (0.0)\"#@param {type:\"string\"}\n",
        "\n",
        "#@markdown Prompt\n",
        "all_title = \"scifi trending on artstation:2\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "def parse_key_frames(string, prompt_parser=None):\n",
        "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
        "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    string: string\n",
        "        Frame numbers paired with parameter values at that frame number, in the format\n",
        "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
        "    prompt_parser: function or None, optional\n",
        "        If provided, prompt_parser will be applied to each string of parameter values.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Frame numbers as keys, parameter values at that frame number as values\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If the input string does not match the expected format.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
        "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
        "\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
        "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
        "    \"\"\"\n",
        "    import re\n",
        "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "    frames = dict()\n",
        "    for match_object in re.finditer(pattern, string):\n",
        "        frame = int(match_object.groupdict()['frame'])\n",
        "        param = match_object.groupdict()['param']\n",
        "        if prompt_parser:\n",
        "            frames[frame] = prompt_parser(param)\n",
        "        else:\n",
        "            frames[frame] = param\n",
        "\n",
        "    if frames == {} and len(string) != 0:\n",
        "        raise RuntimeError('Key Frame string not correctly formatted')\n",
        "    return frames\n",
        "\n",
        "def get_inbetweens(key_frames, integer=False):\n",
        "    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
        "    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
        "    Any values not provided in the input dict are calculated by linear interpolation between\n",
        "    the values of the previous and next provided frames. If there is no previous provided frame, then\n",
        "    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
        "    then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
        "    all frame values are NaN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key_frames: dict\n",
        "        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
        "    integer: Bool, optional\n",
        "        If True, the values of the output series are converted to integers.\n",
        "        Otherwise, the values are floats.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        A Series with length max_frames representing the parameter values for each frame.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> max_frames = 5\n",
        "    >>> get_inbetweens({1: 5, 3: 6})\n",
        "    0    5.0\n",
        "    1    5.0\n",
        "    2    5.5\n",
        "    3    6.0\n",
        "    4    6.0\n",
        "    dtype: float64\n",
        "\n",
        "    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
        "    0    5\n",
        "    1    5\n",
        "    2    5\n",
        "    3    6\n",
        "    4    6\n",
        "    dtype: int64\n",
        "    \"\"\"\n",
        "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "    for i, value in key_frames.items():\n",
        "        key_frame_series[i] = value\n",
        "    key_frame_series = key_frame_series.astype(float)\n",
        "    key_frame_series = key_frame_series.interpolate(limit_direction='both')\n",
        "    if integer:\n",
        "        return key_frame_series.astype(int)\n",
        "    return key_frame_series\n",
        "\n",
        "\n",
        "\n",
        "if key_frames:\n",
        "\n",
        "    try:\n",
        "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `angle` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `angle` as \"\n",
        "            f'\"0: ({angle})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        angle = f\"0: ({angle})\"\n",
        "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "\n",
        "    try:\n",
        "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `zoom` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `zoom` as \"\n",
        "            f'\"0: ({zoom})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        zoom = f\"0: ({zoom})\"\n",
        "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "\n",
        "    try:\n",
        "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `translation_x` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `translation_x` as \"\n",
        "            f'\"0: ({translation_x})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        translation_x = f\"0: ({translation_x})\"\n",
        "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "\n",
        "    try:\n",
        "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "    except RuntimeError as e:\n",
        "        print(\n",
        "            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "            \"formatted `translation_y` correctly for key frames.\\n\"\n",
        "            \"Attempting to interpret `translation_y` as \"\n",
        "            f'\"0: ({translation_y})\"\\n'\n",
        "            \"Please read the instructions to find out how to use key frames \"\n",
        "            \"correctly.\\n\"\n",
        "        )\n",
        "        translation_y = f\"0: ({translation_y})\"\n",
        "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "\n",
        "\n",
        "else:\n",
        "    angle = float(angle)\n",
        "    zoom = float(zoom)\n",
        "    translation_x = float(translation_x)\n",
        "    translation_y = float(translation_y)\n",
        "\n",
        "# make sure you dont get an error when you do the run\n",
        "outputFolder = get_output_folder(outputFolderStatic, choose_diffusion_model, batch_outputFolder, use_batch_outputFolder)\n",
        "try:\n",
        "  print(f\"use_random_settings: {use_random_settings}\")\n",
        "except NameError:\n",
        "  use_random_settings = False\n",
        "  print(f\"use_random_settings: {use_random_settings}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_batch_prompts: {use_batch_prompts}\")\n",
        "except NameError:\n",
        "  use_batch_prompts = False\n",
        "  print(f\"use_batch_prompts: {use_batch_prompts}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_random_prompts: {use_random_prompts}\")\n",
        "except NameError:\n",
        "  use_random_prompts = False\n",
        "  print(f\"use_random_prompts: {use_random_prompts}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_random_init: {use_random_init}\")\n",
        "except NameError:\n",
        "  use_random_init = False\n",
        "  print(f\"use_random_init: {use_random_init}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_batch_init: {use_batch_init}\")\n",
        "except NameError:\n",
        "  use_batch_init = False\n",
        "  print(f\"use_batch_init: {use_batch_init}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Animate"
      ],
      "metadata": {
        "id": "px2U1vZaQgCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Display Rate\n",
        "use_display_rate = False #@param {type:\"boolean\"}\n",
        "display_rate = 20 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Display Percent\n",
        "use_display_percent = False #@param {type:\"boolean\"}\n",
        "display_percent = \"0.6,0.8\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Display Init\n",
        "use_display_init = False #@param {type:\"boolean\"}\n",
        "\n",
        "def config():\n",
        "    vitb32 = lambda: get_clip('ViT-B/32')\n",
        "    vitb16 = lambda: get_clip('ViT-B/16')\n",
        "    vitl14 = lambda: get_clip('ViT-L/14')\n",
        "\n",
        "    if choose_diffusion_model == \"LerpedModels\":\n",
        "        # -- Combine different models to a single output --\n",
        "        \n",
        "        modelsToLerp = []\n",
        "        cond_model = None\n",
        "\n",
        "        if OpenAI512_weight != 0:\n",
        "            openai512Lerp = openai_512()\n",
        "            modelsToLerp.append(openai512Lerp)\n",
        "        if OpenAI256_weight != 0 or choose_cond_model == \"OpenAI256\":\n",
        "            openai256Lerp = openai_256()\n",
        "            modelsToLerp.append(openai256Lerp) if OpenAI256_weight != 0 else None\n",
        "            cond_model = openai256Lerp if choose_cond_model == \"OpenAI256\" else cond_model\n",
        "        if OpenAIFinetune_weight != 0:\n",
        "            openaifinetuneLerp = openai_512_finetune()\n",
        "            modelsToLerp.append(openaifinetuneLerp)\n",
        "        if PixelArtv4_weight != 0 or choose_cond_model == \"PixelArtv4\":\n",
        "            pixelartv4Lerp = pixelartv4_wrap(pixelartv4_params())\n",
        "            modelsToLerp.append(pixelartv4Lerp) if PixelArtv4_weight != 0 else None\n",
        "            cond_model = pixelartv4Lerp if choose_cond_model == \"PixelArtv4\" else cond_model\n",
        "        if PixelArtv6_weight != 0 or choose_cond_model == \"PixelArtv6\":\n",
        "            pixelartv6Lerp = pixelartv6_wrap()\n",
        "            modelsToLerp.append(pixelartv6Lerp) if PixelArtv6_weight != 0 else None\n",
        "            cond_model = pixelartv6Lerp if choose_cond_model == \"PixelArtv6\" else cond_model\n",
        "        if PixelArtv7_weight != 0 or choose_cond_model == \"PixelArtv7\":\n",
        "            cond = jnp.array(TF.to_tensor(Image.open(fetch(ic_cond)).convert('RGB'))) * 2 - 1\n",
        "            cond = jnp.concatenate([cond]*(image_size[1]//cond.shape[-2]+1), axis=-2)[:, :image_size[1], :]\n",
        "            cond = jnp.concatenate([cond]*(image_size[0]//cond.shape[-1]+1), axis=-1)[:, :, :image_size[0]]\n",
        "            cond = cond.broadcast_to([batch_size, 3, image_size[1], image_size[0]])\n",
        "            pixelartv7Lerp = pixelartv7_ic_attn(cond, ic_guidance_scale)\n",
        "            modelsToLerp.append(pixelartv7Lerp) if PixelArtv7_weight != 0 else None\n",
        "            cond_model = pixelartv7Lerp if choose_cond_model == \"PixelArtv7\" else cond_model\n",
        "        if cc12m_weight != 0 or choose_cond_model == \"cc12m\":\n",
        "            cc12mLerp = cc12m_1_wrap(clip_embed=process_prompts(vitb16(),title).squeeze(0) if ('.png' or '.jpg') in all_title else vitb16().embed_texts(process_prompts(vitb16(),title)))\n",
        "            modelsToLerp.append(cc12mLerp) if cc12m_weight != 0 else None\n",
        "            cond_model = cc12mLerp if choose_cond_model == \"cc12m\" else cond_model\n",
        "        if cc12m_cfg_weight != 0 or choose_cond_model == \"cc12m_cfg\":\n",
        "            cc12m_cfgLerp = cc12m_1_cfg_wrap(clip_embed=process_prompts(vitb16(), title), cfg_guidance_scale=local_cfg_guidance_scale)\n",
        "            modelsToLerp.append(cc12m_cfgLerp) if cc12m_cfg_weight != 0 else None\n",
        "            cond_model = cc12m_cfgLerp if choose_cond_model == \"cc12m_cfg\" else cond_model\n",
        "        if WikiArt_weight != 0 or choose_cond_model == \"WikiArt\":\n",
        "            wikiartLerp = wikiart_256()\n",
        "            modelsToLerp.append(wikiartLerp) if WikiArt_weight != 0 else None\n",
        "            cond_model = wikiartLerp if choose_cond_model == \"WikiArt\" else cond_model\n",
        "        if Danbooru_weight != 0 or choose_cond_model == \"Danbooru\":\n",
        "            danbooruLerp = danbooru_128()\n",
        "            modelsToLerp.append(danbooruLerp) if Danbooru_weight != 0 else None\n",
        "            cond_model = danbooruLerp if choose_cond_model == \"Danbooru\" else cond_model\n",
        "        if Imagenet128_weight != 0 or choose_cond_model == \"Imagenet128\":\n",
        "            Imagenet128Lerp = imagenet_128()\n",
        "            modelsToLerp.append(Imagenet128Lerp) if Imagenet128_weight != 0 else None\n",
        "            cond_model = Imagenet128Lerp if choose_cond_model == \"Imagenet128\" else cond_model\n",
        "        if secondary2_weight != 0 or choose_cond_model == \"secondary2\":\n",
        "            secondary2 = secondary2_wrap()\n",
        "            modelsToLerp.append(secondary2) if secondary2_weight != 0 else None\n",
        "            cond_model = secondary2 if choose_cond_model == \"secondary2\" else cond_model\n",
        "        if use_antijpeg:\n",
        "            antiJpegLerp = anti_jpeg_cfg()\n",
        "            modelsToLerp.append(antiJpegLerp)\n",
        "            lerpWeights.append(1.0)\n",
        "            jpeg_classifier_fn = jpeg_classifier_wrap(jpeg_classifier_params(),\n",
        "                                                      guidance_scale=antijpeg_guidance_scale, # will generally depend on image size\n",
        "                                                      flood_level=0.7, # Prevent over-optimization\n",
        "                                                      blur_size=3.0)\n",
        "            \n",
        "        diffusion = LerpModels([(model, weight) for model, weight in zip(modelsToLerp, lerpWeights)])\n",
        "\n",
        "    else:\n",
        "        if choose_diffusion_model == 'OpenAI':\n",
        "          diffusion = openai_512()\n",
        "        if choose_diffusion_model == 'OpenAI256':\n",
        "          diffusion = openai_256()\n",
        "        elif choose_diffusion_model in ('WikiArt', 'Danbooru', 'Imagenet'):\n",
        "          if choose_diffusion_model == 'WikiArt':\n",
        "              diffusion = wikiart_256()\n",
        "          elif choose_diffusion_model == 'Danbooru':\n",
        "              diffusion = danbooru_128()\n",
        "          elif choose_diffusion_model == 'Imagenet':\n",
        "              diffusion = imagenet_128()\n",
        "        elif 'PixelArt' in choose_diffusion_model:\n",
        "          # -- pixel art model --\n",
        "          if choose_diffusion_model == 'PixelArtv7_ic_attn':\n",
        "              cond = jnp.array(TF.to_tensor(Image.open(fetch(ic_cond)).convert('RGB'))) * 2 - 1\n",
        "              cond = jnp.concatenate([cond]*(image_size[1]//cond.shape[-2]+1), axis=-2)[:, :image_size[1], :]\n",
        "              cond = jnp.concatenate([cond]*(image_size[0]//cond.shape[-1]+1), axis=-1)[:, :, :image_size[0]]\n",
        "              cond = cond.broadcast_to([batch_size, 3, image_size[1], image_size[0]])\n",
        "              diffusion = pixelartv7_ic_attn(cond, ic_guidance_scale)\n",
        "          elif choose_diffusion_model == 'PixelArtv6':\n",
        "              diffusion = pixelartv6_wrap(pixelartv6_params())\n",
        "          elif choose_diffusion_model == 'PixelArtv4':\n",
        "              diffusion = pixelartv4_wrap(pixelartv4_params())\n",
        "              diffusion = pixelartv4_wrap(pixelartv4_params())\n",
        "        elif choose_diffusion_model == 'cc12m':\n",
        "          diffusion = cc12m_1_wrap(cc12m_1_params(), clip_embed=process_prompts(vitb16(), title))\n",
        "        elif choose_diffusion_model == 'cc12m_cfg':\n",
        "          diffusion = cc12m_1_cfg_wrap(clip_embed=process_prompts(vitb16(), title), cfg_guidance_scale=local_cfg_guidance_scale)\n",
        "        elif choose_diffusion_model == 'OpenAIFinetune':\n",
        "            diffusion = openai_512_finetune()\n",
        "\n",
        "        if use_secondary_model:\n",
        "          cond_model = secondary2_wrap()\n",
        "        else:\n",
        "          cond_model = diffusion\n",
        "\n",
        "        if use_antijpeg:\n",
        "          diffusion = LerpModels([(diffusion, 1.0),\n",
        "                                  (anti_jpeg_cfg(), 1.0)])\n",
        "          jpeg_classifier_fn = jpeg_classifier_wrap(jpeg_classifier_params(),\n",
        "                                                      guidance_scale=antijpeg_guidance_scale, # will generally depend on image size\n",
        "                                                      flood_level=0.7, # Prevent over-optimization\n",
        "                                                      blur_size=3.0)\n",
        "\n",
        "    if use_antijpeg and (antijpeg_guidance_scale > 0):\n",
        "      cond_fn = CondFns(MainCondFn(cond_model, [\n",
        "        CondCLIP(vitb32(), make_cutouts, cut_batches,\n",
        "                SphericalDistLoss(process_prompts(vitb32(), title), local_clip_guidance_scale) if local_clip_guidance_scale > 0 else None)\n",
        "        if use_vitb32 and local_clip_guidance_scale > 0 else None,\n",
        "\n",
        "        CondCLIP(vitb16(), make_cutouts, cut_batches,\n",
        "                SphericalDistLoss(process_prompts(vitb16(), title), local_clip_guidance_scale) if local_clip_guidance_scale > 0 else None,\n",
        "                AestheticExpected(aesthetic_loss_scale) if aesthetic_loss_scale > 0 else None)\n",
        "        if use_vitb16 and (local_clip_guidance_scale > 0 or aesthetic_loss_scale > 0) else None,\n",
        "\n",
        "        CondCLIP(vitl14(), make_cutouts, cut_batches,\n",
        "                SphericalDistLoss(process_prompts(vitl14(), title), local_clip_guidance_scale) if local_clip_guidance_scale > 0 else None)\n",
        "        if use_vitl14 and local_clip_guidance_scale > 0 else None,\n",
        "\n",
        "        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "        CondMSE(local_init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "        CondRange(range_scale) if range_scale > 0 else None,\n",
        "        CondMean(mean_scale) if mean_scale > 0 else None,\n",
        "        CondVar(var_scale) if var_scale > 0 else None,\n",
        "        CondHorizontalSymmetry(horizontal_symmetry_scale) if horizontal_symmetry_scale > 0 else None,\n",
        "        CondVerticalSymmetry(vertical_symmetry_scale) if vertical_symmetry_scale > 0 else None,\n",
        "      ]), jpeg_classifier_fn)\n",
        "    else:\n",
        "      cond_fn = MainCondFn(cond_model, [\n",
        "        CondCLIP(vitb32(), make_cutouts, cut_batches,\n",
        "                SphericalDistLoss(process_prompts(vitb32(), title), local_clip_guidance_scale) if local_clip_guidance_scale > 0 else None)\n",
        "        if use_vitb32 and local_clip_guidance_scale > 0 else None,\n",
        "\n",
        "        CondCLIP(vitb16(), make_cutouts, cut_batches,\n",
        "                SphericalDistLoss(process_prompts(vitb16(), title), local_clip_guidance_scale) if local_clip_guidance_scale > 0 else None,\n",
        "                AestheticExpected(aesthetic_loss_scale) if aesthetic_loss_scale > 0 else None)\n",
        "        if use_vitb16 and (local_clip_guidance_scale > 0 or aesthetic_loss_scale > 0) else None,\n",
        "\n",
        "        CondCLIP(vitl14(), make_cutouts, cut_batches,\n",
        "                SphericalDistLoss(process_prompts(vitl14(), title), local_clip_guidance_scale) if local_clip_guidance_scale > 0 else None)\n",
        "        if use_vitl14 and local_clip_guidance_scale > 0 else None,\n",
        "\n",
        "        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "        CondMSE(local_init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "        CondRange(range_scale) if range_scale > 0 else None,\n",
        "        CondMean(mean_scale) if mean_scale > 0 else None,\n",
        "        CondVar(var_scale) if var_scale > 0 else None,\n",
        "        CondHorizontalSymmetry(horizontal_symmetry_scale) if horizontal_symmetry_scale > 0 else None,\n",
        "        CondVerticalSymmetry(vertical_symmetry_scale) if vertical_symmetry_scale > 0 else None,\n",
        "      ])\n",
        "\n",
        "    return diffusion, cond_fn\n",
        "\n",
        "def sanitize(title):\n",
        "  return title[:100].replace('/', '_').replace('\\\\', '_')\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_animation(n_frame):\n",
        "    \n",
        "    rng = PRNG(jax.random.PRNGKey(int(time.time())))\n",
        "\n",
        "    for i in range(n_batches):\n",
        "\n",
        "        ts = schedule\n",
        "        alphas, sigmas = cosine.to_alpha_sigma(ts)\n",
        "        print(ts[0], sigmas[0], alphas[0])\n",
        "\n",
        "        x = jax.random.normal(rng.split(), [batch_size, 3, image_size[1], image_size[0]])\n",
        "\n",
        "        if local_init_array is not None:\n",
        "            x = sigmas[0] * x + alphas[0] * local_init_array\n",
        "\n",
        "        # main loop\n",
        "        if sample_mode == 'ddim':\n",
        "          sample_loop = partial(sampler.ddim_sample_loop, eta=eta)\n",
        "        elif sample_mode == 'prk':\n",
        "          sample_loop = sampler.prk_sample_loop\n",
        "        elif sample_mode == 'plms':\n",
        "          sample_loop = sampler.plms_sample_loop\n",
        "        for output in sampler.ddim_sample_loop(diffusion, cond_fn, x, schedule, rng.split(), x_fn = x_transformation):\n",
        "            j = output['step']\n",
        "            pred = output['pred']\n",
        "            assert x.isfinite().all().item()\n",
        "\n",
        "            # display init\n",
        "            if (use_display_init and j == 0) and (local_init_array is not None):\n",
        "              display_images(pred)\n",
        "            \n",
        "            # rate\n",
        "            if ((j % display_rate) == 0 and use_display_rate) and (j not in [0,len(schedule)-1] ):\n",
        "              display_images(pred)\n",
        "\n",
        "            # percent\n",
        "            if ((j in display_steps) and use_display_percent) and j != len(schedule)-1:\n",
        "              display_images(pred)\n",
        "\n",
        "        # save samples\n",
        "        display_images(pred)\n",
        "        images = pred.add(1).div(2).clamp(0, 1)\n",
        "        images = torch.tensor(np.array(images))\n",
        "        for k in range(batch_size):\n",
        "          pil_image = TF.to_pil_image(images[k])\n",
        "          pil_image.save(f'{outputFolder}{timestring}_{str(n_frame).zfill(len(str(max_frames)))}.png')\n",
        "\n",
        "    return(pred)\n",
        "\n",
        "# main\n",
        "try:\n",
        "\n",
        "  # wip batch titles\n",
        "  batch_titles = [all_title]*max_frames\n",
        "\n",
        "  # loop over prompts in batch_titles\n",
        "  for ii in range(max_frames):\n",
        "    print(f\"frame {ii}/{max_frames}\")\n",
        "    local_title = batch_titles[ii]\n",
        "    if ii > 0 and use_prev_frame_image_prompt:\n",
        "      local_title = local_title + \"| firstFrame.png:0.01 | prevFrame.png:0.01\"\n",
        "    \n",
        "    title = expand([local_title], batch_size)\n",
        "\n",
        "    if ii == 0:\n",
        "      # initalize first frame\n",
        "      timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "      local_seed = int(time.time())\n",
        "      save_still_settings(local_seed,outputFolder,timestring)\n",
        "\n",
        "      print(f'Starting run ({timestring}) of ({all_title}) with seed ({local_seed})...')\n",
        "      print(f\"Loading {choose_diffusion_model}...\")\n",
        "\n",
        "      # init\n",
        "      if use_init:\n",
        "        try:\n",
        "          if type(init_image) is list:\n",
        "            init_array = sum(load_image(url) for url in init_image) / len(init_image)\n",
        "          elif type(init_image) is str:\n",
        "            init_array = jnp.concatenate([load_image(it) for it in braceexpand(init_image)], axis=0)\n",
        "          else:\n",
        "            init_array = None\n",
        "        except:\n",
        "          init_array = load_image(init_location+init_image)\n",
        "      \n",
        "      # local noise and init assignment\n",
        "      if use_init:\n",
        "        local_init_array = init_array\n",
        "      else:\n",
        "        local_init_array = None\n",
        "      local_starting_noise = starting_noise\n",
        "      local_skip_percent = skip_percent\n",
        "      local_init_weight_mse = init_weight_mse\n",
        "      local_clip_guidance_scale = clip_guidance_scale\n",
        "      local_cfg_guidance_scale = cfg_guidance_scale\n",
        "      local_ending_noise = ending_noise\n",
        "      local_steps = steps\n",
        "\n",
        "    # next frame\n",
        "    if ii > 0:\n",
        "      #local_init_array = init_array\n",
        "      local_starting_noise = prev_frame_starting_noise\n",
        "      local_skip_percent = prev_frame_skip_percent\n",
        "      local_init_weight_mse = prev_frame_weight_mse\n",
        "      local_clip_guidance_scale = prev_frame_clip_guidance_scale\n",
        "      local_cfg_guidance_scale = prev_frame_cfg_guidance_scale\n",
        "      local_ending_noise = prev_frame_ending_noise\n",
        "      local_steps = steps\n",
        "\n",
        "      # key frames\n",
        "      img_0 = cv2.imread('prevFrame.png')\n",
        "      if key_frames:\n",
        "        angle = angle_series[ii]\n",
        "        zoom = zoom_series[ii]\n",
        "        translation_x = translation_x_series[ii]\n",
        "        translation_y = translation_y_series[ii]\n",
        "        print(f'angle: {angle}')\n",
        "        print(f'zoom: {zoom}')\n",
        "        print(f'translation_x: {translation_x}')\n",
        "        print(f'translation_y: {translation_y}')\n",
        "\n",
        "        center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "        trans_mat = np.float32(\n",
        "                [[1, 0, translation_x],\n",
        "                [0, 1, translation_y]]\n",
        "                )\n",
        "        rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "        trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "        rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "        transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "        img_0 = cv2.warpPerspective(\n",
        "            img_0,\n",
        "            transformation_matrix,\n",
        "            (img_0.shape[1], img_0.shape[0]),\n",
        "            borderMode=cv2.BORDER_WRAP\n",
        "            )\n",
        "      cv2.imwrite('prevFrame.png', img_0)\n",
        "      local_init_array = load_image('prevFrame.png')\n",
        "\n",
        "    # preperation\n",
        "    schedule = jnp.linspace(local_starting_noise, local_ending_noise, local_steps+1)\n",
        "    schedule = spliced.to_cosine(schedule)\n",
        "    if local_skip_percent > 0:\n",
        "      skip_steps = int(local_skip_percent*local_steps)\n",
        "      schedule = schedule[skip_steps:]\n",
        "      local_steps = local_steps-skip_steps\n",
        "\n",
        "    if use_display_percent:\n",
        "      temp = [float(vals) for vals in display_percent.split(\",\")]\n",
        "      display_steps = [int(local_steps*percent) for percent in temp]\n",
        "    else:\n",
        "      display_steps = []\n",
        "\n",
        "    if use_huemin_cuts:\n",
        "      make_cutouts = MakeCutouts_huemin(clip_size, cutn, cut_pow=cut_pow, p_grey=cut_p_grey, p_flip=cut_p_flip, p_mixgrey=cut_p_mixgrey)\n",
        "    else:\n",
        "      make_cutouts = MakeCutouts(clip_size, cutn, cut_pow=cut_pow, p_grey=cut_p_grey, p_flip=cut_p_flip, p_mixgrey=cut_p_mixgrey)\n",
        "    \n",
        "    # transformation functions\n",
        "    transformation_percent = [float(vals) for vals in transformation_schedule.split(\",\")]\n",
        "    transformation_steps = [int(local_steps*i) for i in transformation_percent]\n",
        "    t_schedule = [schedule[i] for i in transformation_steps]\n",
        "\n",
        "    def x_transformation(x,t):\n",
        "      if use_horizontal_symmetry:\n",
        "        if t in t_schedule:\n",
        "          [n, c, h, w] = x.shape\n",
        "          x = jnp.concatenate([x[:, :, :, :w//2], jnp.flip(x[:, :, :, :w//2],-1)], -1)\n",
        "          print(\" horizontal symmetry applied\")\n",
        "      if use_vertical_symmetry:\n",
        "        if t in t_schedule:\n",
        "          [n, c, h, w] = x.shape\n",
        "          x = jnp.concatenate([x[:, :, :h//2, :], jnp.flip(x[:, :, :h//2, :],-2)], -2)\n",
        "          print(\" vertical symmetry applied\")\n",
        "      return x\n",
        "\n",
        "    # config run\n",
        "    diffusion, cond_fn = config()\n",
        "    \n",
        "    # run\n",
        "    pred = run_animation(ii)\n",
        "\n",
        "    # convert pred to image\n",
        "    images = pred.add(1).div(2).clamp(0, 1)\n",
        "    images = torch.tensor(np.array(images))\n",
        "    pil_image = TF.to_pil_image(images[0])\n",
        "    pil_image.save('prevFrame.png')\n",
        "    if ii == 0:\n",
        "      pil_image.save('firstFrame.png')\n",
        "\n",
        "    success = True\n",
        "\n",
        "except:\n",
        "  import traceback\n",
        "  traceback.print_exc()\n",
        "  success = False\n",
        "assert success"
      ],
      "metadata": {
        "id": "u1sWbA1zRFKb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export"
      ],
      "metadata": {
        "id": "_phMygappxgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### **Create Frame Sequence**\n",
        "\n",
        "# sequence parameters\n",
        "run_ids = [20220409213802] #@param # list of timestrings in order\n",
        "start_frames = [0] #@param # list of start frames in order\n",
        "stop_frames = [3] #@param # list of stop frames in order\n",
        "# max_frames must match the runs max_frames\n",
        "\n",
        "# verify correct lengths\n",
        "if len(run_ids) == len(start_frames) and len(start_frames) == len(stop_frames):\n",
        "\n",
        "  # frame sequence\n",
        "  frames = []\n",
        "\n",
        "  # expand list\n",
        "  for ii in range(len(run_ids)):\n",
        "    tmp_frames = [str(run_ids[ii])+\"_\"+str(i).zfill(len(str(max_frames)))+\".png\" for i in list(range(start_frames[ii],stop_frames[ii]+1))]\n",
        "    frames = frames + tmp_frames\n",
        "\n",
        "ffmpeg_concat = \"/\".join(outputFolder.split(\"/\")[2:])\n",
        "\n",
        "# save frames to txt file\n",
        "text_file = open(\"sample.txt\", \"w\")\n",
        "for ii in range(len(frames)):\n",
        "  tmp_line = \"file '\"+outputFolder+frames[ii]+\"'\\n\"\n",
        "  n = text_file.write(tmp_line)\n",
        "text_file.close()\n",
        "\n",
        "tmp_df = pd.DataFrame({\"frames\":frames})\n",
        "print(tmp_df)"
      ],
      "metadata": {
        "id": "-Punf6oC1RXh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### **Create Video From Fame Sequence**\n",
        "\n",
        "import glob\n",
        "\n",
        "# make video output folder path\n",
        "videoOutputFolder = outputFolder+\"videos/\"\n",
        "os.makedirs(videoOutputFolder, exist_ok=True)\n",
        "\n",
        "# import subprocess in case this cell is run without the above cells\n",
        "import subprocess\n",
        "from base64 import b64encode\n",
        "\n",
        "fps = 30#@param {type:\"number\"}\n",
        "make_mp4 = False #@param {type: 'boolean'}\n",
        "make_gif = True #@param {type: 'boolean'}\n",
        "\n",
        "\n",
        "frames = []\n",
        "  # tqdm.write('Generating video...')\n",
        "\n",
        "if make_mp4:\n",
        "  filepath = f\"{videoOutputFolder}{time.strftime('%Y%m%d%H%M%S')}.mp4\"\n",
        "\n",
        "  cmd = [\n",
        "      'ffmpeg',\n",
        "      '-y',\n",
        "      '-vcodec',\n",
        "      'png',\n",
        "      '-r',\n",
        "      str(fps),\n",
        "      '-f',\n",
        "      'concat',\n",
        "      '-safe',\n",
        "      '0',\n",
        "      '-i',\n",
        "      '/content/sample.txt',\n",
        "      '-c:v',\n",
        "      'libx264',\n",
        "      '-vf',\n",
        "      f'fps={fps}',\n",
        "      '-pix_fmt',\n",
        "      'yuv420p',\n",
        "      '-crf',\n",
        "      '17',\n",
        "      '-preset',\n",
        "      'veryslow',\n",
        "      filepath\n",
        "    ]\n",
        "\n",
        "elif make_gif:\n",
        "  filepath = f\"{videoOutputFolder}{time.strftime('%Y%m%d%H%M%S')}.gif\"\n",
        "\n",
        "  cmd = [\n",
        "      'ffmpeg',\n",
        "      '-y',\n",
        "      '-vcodec',\n",
        "      'png',\n",
        "      '-r',\n",
        "      str(fps),\n",
        "      '-f',\n",
        "      'concat',\n",
        "      '-safe',\n",
        "      '0',\n",
        "      '-i',\n",
        "      '/content/sample.txt',\n",
        "      '-vf',\n",
        "      f'fps={fps}',\n",
        "      filepath\n",
        "    ]\n",
        "\n",
        "process = subprocess.Popen(cmd, cwd=f'{outputFolder}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    print(stderr.decode(\"utf-8\"))\n",
        "    raise RuntimeError(stderr)\n",
        "else:\n",
        "    if make_mp4:\n",
        "      print(\"mp4 is ready\")\n",
        "    elif make_gif:\n",
        "      print(\"gif is ready\")\n",
        "        "
      ],
      "metadata": {
        "id": "Su2GF5iyQxRO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### **Create video (old method if the cells above cause problems)**\n",
        "#@markdown Video file will save in the same folder as your images.\n",
        "\n",
        "import glob\n",
        "skip_video_for_run_all = False #@param {type: 'boolean'}\n",
        "\n",
        "if skip_video_for_run_all == False:\n",
        "  # import subprocess in case this cell is run without the above cells\n",
        "  import subprocess\n",
        "  from base64 import b64encode\n",
        "\n",
        "  latest_run = timestring\n",
        "\n",
        "  use_latest_run = True #@param {type: 'boolean'}\n",
        "  folder = batch_outputFolder #@param\n",
        "  run_id = latest_run #@param\n",
        "  if use_latest_run:\n",
        "    run_id = timestring\n",
        "\n",
        "  init_frame = 0#@param {type:\"number\"} This is the frame where the video will start\n",
        "  last_frame = 63#@param {type:\"number\"} You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "  fps = 12#@param {type:\"number\"}\n",
        "  view_video_in_cell = False #@param {type: 'boolean'}\n",
        "\n",
        "  frames = []\n",
        "  # tqdm.write('Generating video...')\n",
        "\n",
        "  image_path = f\"{outputFolder}{run_id}_%04d.png\"\n",
        "  filepath = f\"{outputFolder}{run_id}.mp4\"\n",
        "\n",
        "\n",
        "  cmd = [\n",
        "      'ffmpeg',\n",
        "      '-y',\n",
        "      '-vcodec',\n",
        "      'png',\n",
        "      '-r',\n",
        "      str(fps),\n",
        "      '-start_number',\n",
        "      str(init_frame),\n",
        "      '-i',\n",
        "      image_path,\n",
        "      '-frames:v',\n",
        "      str(last_frame),\n",
        "      '-c:v',\n",
        "      'libx264',\n",
        "      '-vf',\n",
        "      f'fps={fps}',\n",
        "      '-pix_fmt',\n",
        "      'yuv420p',\n",
        "      '-crf',\n",
        "      '17',\n",
        "      '-preset',\n",
        "      'veryslow',\n",
        "      filepath\n",
        "  ]\n",
        "\n",
        "  process = subprocess.Popen(cmd, cwd=f'{outputFolder}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  stdout, stderr = process.communicate()\n",
        "  if process.returncode != 0:\n",
        "      print(stderr)\n",
        "      raise RuntimeError(stderr)\n",
        "  else:\n",
        "      print(\"The video is ready\")\n",
        "\n",
        "  if view_video_in_cell:\n",
        "      mp4 = open(filepath,'rb').read()\n",
        "      data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "      display.HTML(\"\"\"\n",
        "      <video width=400 controls>\n",
        "            <source src=\"%s\" type=\"video/mp4\">\n",
        "      </video>\n",
        "      \"\"\" % data_url)"
      ],
      "metadata": {
        "id": "cZ7_6yN_SMUG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}