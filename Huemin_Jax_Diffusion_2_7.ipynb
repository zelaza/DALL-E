{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Huemin Jax Diffusion 2.7\n",
        "thank you [RiversHaveWings](https://twitter.com/RiversHaveWings) and [nshepperd](https://twitter.com/nshepperd1) for making this all possible"
      ],
      "metadata": {
        "id": "u65wfW8-Ng3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D-DD0F0u_SY"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DcaT389kvakt"
      },
      "outputs": [],
      "source": [
        "#@markdown Default Path Variables:\n",
        "models_path = \"/content/models/\" #@param {type:\"string\"}\n",
        "output_path = \"/content/output/\" #@param {type:\"string\"}\n",
        "link_path = \"/content/link/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Mount Google Drive (Optional):\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "force_remount = False #@param {type:\"boolean\"}\n",
        "\n",
        "if mount_google_drive:\n",
        "  from google.colab import drive\n",
        "  try:\n",
        "    drive_path = \"/content/drive\" #@param {type:\"string\"}\n",
        "    drive.mount(drive_path,force_remount=force_remount)\n",
        "    models_path = \"/content/drive/MyDrive/AI/models/\" #@param {type:\"string\"}\n",
        "    output_path = \"/content/drive/MyDrive/AI/JAX/\" #@param {type:\"string\"}\n",
        "    link_path = \"/content/drive/MyDrive/AI/JAX/link/\" #@param {type:\"string\"}\n",
        "  except:\n",
        "    print(\"...error mounting drive or with drive path variables\")\n",
        "    print(\"...reverting to default path variables\")\n",
        "    models_path = \"/content/models/\"\n",
        "    output_path = \"/content/output/\"\n",
        "    link_path = \"/content/link/\"    \n",
        "\n",
        "!mkdir -p $models_path\n",
        "!mkdir -p $output_path\n",
        "!mkdir -p $link_path\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")\n",
        "print(f\"link_path: {link_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Descriptions"
      ],
      "metadata": {
        "id": "Gki89ARz6h5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter | Description | Values\n",
        "--- | --- | ---\n",
        "**Run Settings (Required)**\n",
        "`seed` | key for pseudo random number generator | `0` for random\n",
        "`batch_size` | number of images to generate in parallel | `1` recommended \n",
        "`batch_folder` | name of the subfolder for saving iamges | `string`\n",
        "`save_settings` | option to save run settings as .txt file | `True`/`False`\n",
        "**Diffusion Model Settings (Required)**\n",
        "`steps` | number of diffusion steps | `integer`\n",
        "`image_size` | image size in pixels, must be divisible by 64 | `(width,height)`\n",
        "`diffusion_model` | CC12M (256x256) or OpenAI (512x512) | `choose`\n",
        "`use_secondary_model` | lowers vram usage and vram quality | `True`/`False`\n",
        "`sample_mode` | ddim, plms, prk | `choose`\n",
        "`ddim_eta` | only applies to ddim sample mode | `range -1.0 to 1.0`\n",
        "`starting_noise` | 1.0 is max noise 0.0 is no noise  | `range 1.0 to 0.0`\n",
        "`ending_noise` | must be less than starting noise | `range 1.0 to 0.0`\n",
        "**CLIP Model Settings (Required)**\n",
        "`use_vitb32` | safe with most gpus | `True`/`False`\n",
        "`use_vitb16` | safe with most gpus | `True`/`False`\n",
        "`use_vitl14` | uses more vram | `True`/`False`\n",
        "`use_vitl14_336px` | uses a lot more vram | `True`/`False`\n",
        "`all_prompts` | can be text, path, url separate with `\\|` add weights with `:` followed by float | `string`\n",
        "`all_clip_guidance_scale` | applies to each CLIP model | `any number`\n",
        "**Cut Settings (Required)**\n",
        "`cutn` | must be integer divisible by 2 | `integer`\n",
        "`cut_batches` | effective cuts is `cutn` times `cut_batches` | `integer`\n",
        "`cut_pow` | larger cut_pow smaller the cuts | `range 0.0 to 1.0`\n",
        "`cut_p_grey` | percentage of greyscale cuts | `range 0.0 to 1.0`\n",
        "`cut_p_flip` | percentage of flip cuts | `range 0.0 to 1.0`\n",
        "`cut_p_mixgrey` | percentage of partial greyscale | `range 0.0 to 1.0`\n",
        "**Conditonal Settings (Required)**\n",
        "`tv_scale` | more positive means smooth image| `any number`\n",
        "`range_scale` | more positive means keep pixel values in range | `any number`\n",
        "`mean_scale` | more positive means closer to grey | `any number`\n",
        "`var_scale` | more positive means less pixel variation | `any number`\n",
        "`horizontal_symmetry_scale` | more positive means more horizontal symmetry | `any number`\n",
        "`vertical_symmetry_scale` | more positive means more vertical symmetry | `any number`\n",
        "**Multiple Prompt Settings (Optional)**\n",
        "`use_multiple_prompts` | enables multiple separate prompts | `True`/`False`\n",
        "`vitb32_all_prompt` | same as `all_prompt` | `string`\n",
        "`vitb32_clip_guidance_scale` | same as `all_clip_guidance_scale` | `any number`\n",
        "`vitb16_all_prompt` | same as `all_prompt` | `string`\n",
        "`vitb16_clip_guidance_scale` | same as `all_clip_guidance_scale` | `any number`\n",
        "`vitl14_all_prompt` | same as `all_prompt` | `string`\n",
        "`vitl14_clip_guidance_scale` | same as `all_clip_guidance_scale` | `any number`\n",
        "`vitl14_336px_all_prompt` | same as `all_prompt` | `string`\n",
        "`vitl14_336px_clip_guidance_scale` | same as `all_clip_guidance_scale` | `any number`\n",
        "**Initial Image Settings (Optional)**\n",
        "`use_init_img` | enables inital image | `True`/`False`\n",
        "`init_path` | the name or path to an initial image | `string`\n",
        "`use_mse_img` | enables separate mse image | `True`/`False`\n",
        "`init_mse_path` | the name or path to an mse image | `string`\n",
        "`init_mse_scale` | more positive means closer to the mse image | `any number`\n",
        "`use_masked_mse` | enables inital image mask | `True`/`False`\n",
        "`init_mse_alpha_path` | the name or path to an alpha mask | `string`\n",
        "`init_append_path` | path prefix such that `append_path` + `path` = `full_path` | `string`\n",
        "**Transformation Settings (Optional)**\n",
        "`use_vertical_symmetry` | enables vertical symmetry transformation | `True`/`False`\n",
        "`use_horizontal_symmetry` | enables horizontal symmetry transformation | `True`/`False`\n",
        "`transformation_schedule` | list of percentages from 0.0 to 1.0 specifying when the transformation is performed | `list`\n"
      ],
      "metadata": {
        "id": "u6PrH18X4H_C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ5aW34zTJBX"
      },
      "source": [
        "# Setup + Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoI_KUhtTTer"
      },
      "source": [
        "download repositories and patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24bGVHTgTDrW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.system(\"nvidia-smi | grep A100\") == 0:\n",
        "  !pip install -U https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.72+cuda111-cp37-none-manylinux2010_x86_64.whl \"jax==0.2.25\"\n",
        "else:\n",
        "  !pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.75%2Bcuda11.cudnn805-cp37-none-manylinux2010_x86_64.whl \"jax==0.3.0\"\n",
        "!pip install dm-haiku==0.0.5 cbor2 ftfy einops braceexpand\n",
        "!git clone https://github.com/enzymezoo-code/CLIP_JAX\n",
        "!git clone https://github.com/huemin-art/jax-guided-diffusion -b v2.7\n",
        "!git clone https://github.com/huemin-art/v-diffusion-jax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw_jQBhoTiMO"
      },
      "source": [
        "import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l213ok1iS4H1"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append('./CLIP_JAX')\n",
        "sys.path.append('./jax-guided-diffusion')\n",
        "sys.path.append('./v-diffusion-jax')\n",
        "\n",
        "import io\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "from subprocess import Popen\n",
        "from functools import partial\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jaxtorch import PRNG\n",
        "\n",
        "import torch\n",
        "from torch import tensor\n",
        "from torchvision import utils\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "from diffusion_models.schedules import cosine, spliced\n",
        "from diffusion_models.common import norm1, blur_fft, make_partial\n",
        "from diffusion_models.lazy import LazyParams\n",
        "from diffusion_models.perceptor import get_clip, normalize\n",
        "from diffusion_models.cc12m_1 import cc12m_1_wrap\n",
        "from diffusion_models.openai import openai_512_finetune\n",
        "from diffusion_models.secondary import secondary2_wrap\n",
        "from diffusion_models import sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiKk9OvRTpA1"
      },
      "source": [
        "fetch functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B35XNFIZWXrY"
      },
      "outputs": [],
      "source": [
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    local_path = os.path.join(models_path, basename)\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    else:\n",
        "        os.makedirs(f'{models_path}tmp', exist_ok=True)\n",
        "        Popen(['curl', '--http1.1', url_or_path, '-o', f'{models_path}tmp/{basename}']).wait()\n",
        "        os.rename(f'{models_path}tmp/{basename}', local_path)\n",
        "        return local_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVFwGKQ3T3x_"
      },
      "source": [
        "CC12M model parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cc12m_1_params = LazyParams.pt('https://the-eye.eu/public/AI/models/v-diffusion/cc12m_1.pth')\n",
        "LazyParams.fetch = fetch_model"
      ],
      "metadata": {
        "id": "8Cz5g2FoT2bP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4I_dOctT-f4"
      },
      "source": [
        "CLIP cutout functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6VSEAlLZBgD"
      },
      "outputs": [],
      "source": [
        "def grey(image):\n",
        "    [*_, c, h, w] = image.shape\n",
        "    return jnp.broadcast_to(image.mean(axis=-3, keepdims=True), image.shape)\n",
        "\n",
        "def cutout_image(image, offsetx, offsety, size, output_size=224):\n",
        "    \"\"\"Computes (square) cutouts of an image given x and y offsets and size.\"\"\"\n",
        "    (c, h, w) = image.shape\n",
        "\n",
        "    scale = jnp.stack([output_size / size, output_size / size])\n",
        "    translation = jnp.stack([-offsety * output_size / size, -offsetx * output_size / size])\n",
        "    return jax.image.scale_and_translate(image,\n",
        "                                         shape=(c, output_size, output_size),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def cutouts_images(image, offsetx, offsety, size, output_size=224):\n",
        "    f = partial(cutout_image, output_size=output_size)         # [c h w] [] [] [] -> [c h w]\n",
        "    f = jax.vmap(f, in_axes=(0, 0, 0, 0), out_axes=0)          # [n c h w] [n] [n] [n] -> [n c h w]\n",
        "    f = jax.vmap(f, in_axes=(None, 0, 0, 0), out_axes=0)       # [n c h w] [k n] [k n] [k n] -> [k n c h w]\n",
        "    return f(image, offsetx, offsety, size)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.0, p_grey=0.2, p_mixgrey=None, p_flip=0.5):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.p_grey = p_grey\n",
        "        self.p_mixgrey = p_mixgrey\n",
        "        self.p_flip = p_flip\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "\n",
        "        small_cuts = self.cutn//2\n",
        "        large_cuts = self.cutn - self.cutn//2\n",
        "\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[small_cuts, n])**self.cut_pow\n",
        "        sizes = (min_size + cut_us * (max_size - min_size)).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=w - sizes)\n",
        "        offsets_y = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=h - sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes, output_size=self.cut_size)\n",
        "\n",
        "        B1 = 40\n",
        "        B2 = 40\n",
        "        lcut_us = jax.random.uniform(rng.split(), shape=[large_cuts, n])\n",
        "        border = B1 + lcut_us * B2\n",
        "        lsizes = (max(h,w) + border).astype(jnp.int32)\n",
        "        loffsets_x = jax.random.uniform(rng.split(), [large_cuts, n], minval=w/2-lsizes/2-border, maxval=w/2-lsizes/2+border)\n",
        "        loffsets_y = jax.random.uniform(rng.split(), [large_cuts, n], minval=h/2-lsizes/2-border, maxval=h/2-lsizes/2+border)\n",
        "        lcutouts = cutouts_images(input, loffsets_x, loffsets_y, lsizes, output_size=self.cut_size)\n",
        "\n",
        "        cutouts = jnp.concatenate([cutouts, lcutouts], axis=0)\n",
        "\n",
        "        greyed = grey(cutouts)\n",
        "\n",
        "        if self.p_mixgrey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          grey_rs = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_mixgrey, grey_rs * greyed + (1 - grey_rs) * cutouts, cutouts)\n",
        "\n",
        "        if self.p_grey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_grey, greyed, cutouts)\n",
        "\n",
        "        if self.p_flip is not None:\n",
        "          flip_us = jax.random.bernoulli(rng.split(), self.p_flip, [self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(flip_us, jnp.flip(cutouts, axis=-1), cutouts)\n",
        "\n",
        "        return cutouts\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.cut_pow, self.p_grey, self.p_mixgrey, self.p_flip], (self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        return MakeCutouts(cut_size, cutn, *dynamic)\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = norm1(x)\n",
        "    y = norm1(y)\n",
        "    return (x - y).square().sum(axis=-1).sqrt().div(2).arcsin().square().mul(2)\n",
        "\n",
        "@make_partial\n",
        "def SphericalDistLoss(text_embed, clip_guidance_scale, image_embeds):\n",
        "    losses = spherical_dist_loss(image_embeds, text_embed).mean(0)\n",
        "    return (clip_guidance_scale * losses).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkscEaSeUIN3"
      },
      "source": [
        "process prompt function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHJNSGTtV1Lv"
      },
      "outputs": [],
      "source": [
        "def process_prompt(clip,all_prompt):\n",
        "  embeds = []\n",
        "  expands = all_prompt.split(\"|\")\n",
        "  for prompt in expands:\n",
        "    prompt = prompt.strip()\n",
        "    # check url\n",
        "    if \"https:\" in prompt:\n",
        "      tmp = prompt.split(\":\")\n",
        "      # check weight\n",
        "      if len(tmp) == 2:\n",
        "        temp_weight = 1\n",
        "        temp_prompt = prompt\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here1\")\n",
        "        #print(tmp_embed.shape)\n",
        "      if len(tmp) == 3:\n",
        "        temp_prompt = \":\".join(tmp[0:2]).strip()\n",
        "        temp_weight = float(tmp[2].strip())\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here2\")\n",
        "        #print(tmp_embed.shape)\n",
        "    # if not url\n",
        "    else:\n",
        "      # check weight\n",
        "      if ':' in prompt:\n",
        "        tmp = prompt.split(\":\")\n",
        "        temp_prompt = tmp[0].strip()\n",
        "        temp_weight = float(tmp[1].strip())\n",
        "      else:\n",
        "        temp_prompt = prompt\n",
        "        temp_weight = 1\n",
        "      # try path\n",
        "      try:\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "      except:\n",
        "        tmp_embed = temp_weight * clip.embed_text(temp_prompt.strip())\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here4\")\n",
        "        #print(tmp_embed.shape)\n",
        "  return norm1(sum(embeds))\n",
        "\n",
        "def process_prompts(clip, prompts):\n",
        "  return jnp.stack([process_prompt(clip, prompt) for prompt in prompts])\n",
        "\n",
        "def expand(xs, batch_size):\n",
        "  \"\"\"Extend or truncate the list of prompts to the batch size.\"\"\"\n",
        "  return (xs * batch_size)[:batch_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCY3_TLWUOAB"
      },
      "source": [
        "conditional functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjtApGjuWtvn"
      },
      "outputs": [],
      "source": [
        "@jax.tree_util.register_pytree_node_class\n",
        "class MainCondFn(object):\n",
        "    # Used to construct the main cond_fn. Accepts a diffusion model which will\n",
        "    # be used for denoising, plus a list of 'conditions' which will\n",
        "    # generate gradient of a loss wrt the denoised, to be summed together.\n",
        "    def __init__(self, diffusion, conditions, blur_amount=None, use='pred'):\n",
        "        self.diffusion = diffusion\n",
        "        self.conditions = [c for c in conditions if c is not None]\n",
        "        self.blur_amount = blur_amount\n",
        "        self.use = use\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, x, cosine_t, key):\n",
        "        if not self.conditions:\n",
        "          return jnp.zeros_like(x)\n",
        "\n",
        "        rng = PRNG(key)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        alphas, sigmas = cosine.to_alpha_sigma(cosine_t)\n",
        "\n",
        "        def denoise(key, x):\n",
        "            pred = self.diffusion(x, cosine_t, key).pred\n",
        "            if self.use == 'pred':\n",
        "                return pred\n",
        "            elif self.use == 'x_in':\n",
        "                return pred * sigmas + x * alphas\n",
        "        (x_in, backward) = jax.vjp(partial(denoise, rng.split()), x)\n",
        "\n",
        "        total = jnp.zeros_like(x_in)\n",
        "        for cond in self.conditions:\n",
        "            total += cond(x_in, rng.split())\n",
        "        if self.blur_amount is not None:\n",
        "          blur_radius = (self.blur_amount * sigmas / alphas).clamp(0.05,512)\n",
        "          total = blur_fft(total, blur_radius.mean())\n",
        "        final_grad = -backward(total)[0]\n",
        "\n",
        "        # clamp gradients to a max of 0.2\n",
        "        magnitude = final_grad.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "        final_grad = final_grad * jnp.where(magnitude > 0.2, 0.2 / magnitude, 1.0)\n",
        "        return final_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.diffusion, self.conditions, self.blur_amount], [self.use]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MainCondFn(*dynamic, *static)\n",
        "\n",
        "def filternone(xs):\n",
        "  return [x for x in xs if x is not None]\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondCLIP(object):\n",
        "    \"\"\"Backward a loss function through clip.\"\"\"\n",
        "    def __init__(self, perceptor, make_cutouts, cut_batches, *losses):\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.cut_batches = cut_batches\n",
        "        self.losses = filternone(losses)\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts).rearrange('(k n) c -> k n c', n=n)\n",
        "            return sum(loss_fn(image_embeds) for loss_fn in self.losses)\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.perceptor, self.make_cutouts, self.losses], [self.cut_batches]\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        [perceptor, make_cutouts, losses] = dynamic\n",
        "        [cut_batches] = static\n",
        "        return cls(perceptor, make_cutouts, cut_batches, *losses)\n",
        "\n",
        "@make_partial\n",
        "def CondTV(tv_scale, x_in, key):\n",
        "    def downscale2d(image, f):\n",
        "        [c, n, h, w] = image.shape\n",
        "        return jax.image.resize(image, [c, n, h//f, w//f], method='cubic')\n",
        "\n",
        "    def tv_loss(input):\n",
        "        \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "        x_diff = input[..., :, 1:] - input[..., :, :-1]\n",
        "        y_diff = input[..., 1:, :] - input[..., :-1, :]\n",
        "        return x_diff.square().mean([1,2,3]) + y_diff.square().mean([1,2,3])\n",
        "\n",
        "    def sum_tv_loss(x_in, f=None):\n",
        "        if f is not None:\n",
        "            x_in = downscale2d(x_in, f)\n",
        "        return tv_loss(x_in).sum() * tv_scale\n",
        "    tv_grad_512 = jax.grad(sum_tv_loss)(x_in)\n",
        "    tv_grad_256 = jax.grad(partial(sum_tv_loss,f=2))(x_in)\n",
        "    tv_grad_128 = jax.grad(partial(sum_tv_loss,f=4))(x_in)\n",
        "    return tv_grad_512 + tv_grad_256 + tv_grad_128\n",
        "\n",
        "@make_partial\n",
        "def CondRange(range_scale, x_in, key):\n",
        "    def range_loss(x_in):\n",
        "        return jnp.abs(x_in - x_in.clamp(minval=-1,maxval=1)).mean()\n",
        "    return range_scale * jax.grad(range_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMean(mean_scale, x_in, key):\n",
        "    def mean_loss(x_in):\n",
        "        return jnp.abs(x_in).mean()\n",
        "    return mean_scale * jax.grad(mean_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondVar(var_scale, x_in, key):\n",
        "    def var_loss(x_in):\n",
        "        return x_in.var()\n",
        "    return var_scale * jax.grad(var_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondHorizontalSymmetry(horizontal_symmetry_scale, x_in, key):\n",
        "    def horizontal_symmetry_loss(x_in):\n",
        "        [n, c, h, w] = x_in.shape\n",
        "        return jnp.abs(x_in[:, :, :, :w//2]-jnp.flip(x_in[:, :, :, w//2:],-1)).mean()\n",
        "    return horizontal_symmetry_scale * jax.grad(horizontal_symmetry_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondVerticalSymmetry(vertical_symmetry_scale, x_in, key):\n",
        "    def vertical_symmetry_loss(x_in):\n",
        "        [n, c, h, w] = x_in.shape\n",
        "        return jnp.abs(x_in[:, :, :h//2, :]-jnp.flip(x_in[:, :, h//2:, :],-2)).mean()\n",
        "    return vertical_symmetry_scale * jax.grad(vertical_symmetry_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMSE(target, mse_scale, x_in, key):\n",
        "    def mse_loss(x_in):\n",
        "        return (x_in - target).square().mean()\n",
        "    return mse_scale * jax.grad(mse_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMaskedMSE(target, mse_scale, mask, x_in, key):\n",
        "    def mse_loss(x_in):\n",
        "        return (mask * (x_in - target).square()).mean()\n",
        "    return mse_scale * jax.grad(mse_loss)(x_in)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WqbJ2ghUTX7"
      },
      "source": [
        "QoL functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqeaRRlVeQ0p"
      },
      "outputs": [],
      "source": [
        "def display_images(images):\n",
        "  images = images.add(1).div(2).clamp(0, 1)\n",
        "  images = torch.tensor(np.array(images))\n",
        "  grid = utils.make_grid(images, 4).cpu()\n",
        "  display.display(TF.to_pil_image(grid))\n",
        "  return\n",
        "\n",
        "def load_image(url,image_size,resize=True):\n",
        "    init_array = Image.open(fetch(url)).convert('RGB')\n",
        "    if resize:\n",
        "      init_array = init_array.resize(image_size, Image.LANCZOS)\n",
        "    init_array = jnp.array(TF.to_tensor(init_array)).unsqueeze(0).mul(2).sub(1)\n",
        "    return init_array\n",
        "\n",
        "def get_output_folder(output_path,batch_folder=None):\n",
        "  yearMonth = time.strftime('%Y-%m/')\n",
        "  out_path = output_path+yearMonth\n",
        "  if batch_folder != \"\":\n",
        "    out_path += batch_folder\n",
        "    if out_path[-1] != \"/\":\n",
        "      out_path += \"/\"\n",
        "  os.makedirs(out_path, exist_ok=True)\n",
        "  return out_path\n",
        "\n",
        "def print_time_remaining():\n",
        "  tmp = toc - tic\n",
        "  mult_total = len(range(as_n_start,as_n_pass))*len(range(xx_start,len(x_schedule)))*len(range(yy_start,len(y_schedule)))\n",
        "  mult_complete = (nn+1)*(ix+1)*(iy+1)\n",
        "  mult = mult_total - mult_complete\n",
        "  tmp = tmp*mult\n",
        "  if tmp < 60:\n",
        "    unit = 's'\n",
        "  elif tmp < 60*60:\n",
        "    unit = 'mins'\n",
        "    tmp = tmp/60\n",
        "  elif tmp < 60*60*24:\n",
        "    unit = 'hrs'\n",
        "    tmp = tmp/(60*60)\n",
        "  print(f\"...estimated time remaining {tmp:0.2f} {unit} remaining\")\n",
        "  return\n",
        "\n",
        "def strip_weights_from_prompt(all_prompt):\n",
        "  stripped_prompt = []\n",
        "  expands = all_prompt.split(\"|\")\n",
        "  for prompt in expands:\n",
        "    prompt = prompt.strip()\n",
        "    if \"https:\" in prompt:\n",
        "      tmp = prompt.split(\":\")\n",
        "      if len(tmp) == 2:\n",
        "        stripped_prompt.append(prompt)\n",
        "      if len(tmp) == 3:\n",
        "        temp_prompt = \":\".join(tmp[0:2]).strip()\n",
        "        temp_prompt = \":\".join(tmp[0:2]).strip()+\" :\"+tmp[2].strip()\n",
        "        stripped_prompt.append(temp_prompt)\n",
        "    else:\n",
        "      stripped_prompt.append(prompt)\n",
        "\n",
        "  return(\" | \".join(stripped_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AR functions"
      ],
      "metadata": {
        "id": "X-XBb_zow_MK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zxha1Y_f2QQ"
      },
      "outputs": [],
      "source": [
        "@jax.tree_util.register_pytree_node_class\n",
        "class asMakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "\n",
        "        cut_us = jnp.ones((self.cutn,1))\n",
        "        sizes = (cut_us * self.cut_size).clamp(self.cut_size)\n",
        "        offsets_x = jax.random.randint(rng.split(), [self.cutn, n], minval=-self.cut_size//2, maxval=w-self.cut_size//2)\n",
        "        offsets_y = jax.random.randint(rng.split(), [self.cutn, n], minval=-self.cut_size//2, maxval=h-self.cut_size//2)\n",
        "        print(offsets_x)\n",
        "        print(offsets_y)\n",
        "        print(sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes, output_size=self.cut_size)\n",
        "\n",
        "        return cutouts, offsets_x, offsets_y\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ((self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        return asMakeCutouts(cut_size, cutn, *dynamic)\n",
        "\n",
        "def as_cutout_image(ar_image, offsetx, offsety, image_size):\n",
        "    c, h, w = ar_image[0].shape\n",
        "    scale = jnp.stack([1,1])\n",
        "    translation = jnp.stack([-offsety, -offsetx])\n",
        "    return jax.image.scale_and_translate(ar_image[0],\n",
        "                                         shape=(c, image_size[1], image_size[0]),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def create_as_alpha(image_size,border,blur_std):\n",
        "  if border > min(image_size):\n",
        "    print(\"border greater than image dimensions\")\n",
        "  if border < 0:\n",
        "    print(\"border should not be negative\")\n",
        "  ar_alpha = jnp.ones((1,3,image_size[1],image_size[0])).clamp(0,1)\n",
        "  ar_alpha = ar_alpha.at[:,:,:border,:].set(-1*ar_alpha[:,:,:border,:])\n",
        "  ar_alpha = ar_alpha.at[:,:,image_size[1]-border:,:].set(-1*ar_alpha[:,:,image_size[1]-border:,:])\n",
        "  ar_alpha = ar_alpha.at[:,:,border:image_size[1]-border,:border].set(-1*ar_alpha[:,:,border:image_size[1]-border,:border])\n",
        "  ar_alpha = ar_alpha.at[:,:,border:image_size[1]-border,image_size[0]-border:].set(-1*ar_alpha[:,:,border:image_size[1]-border,image_size[0]-border:])\n",
        "  ar_alpha = blur_fft(ar_alpha,blur_std)\n",
        "  return ar_alpha\n",
        "\n",
        "def create_as_offsets(image_size,ar_image_size,overlap_border):\n",
        "  w, h, ar_w, ar_h = image_size[0], image_size[1], ar_image_size[0], ar_image_size[1]\n",
        "  tmp_w = ar_w+overlap_border\n",
        "  tmp_h = ar_h+overlap_border\n",
        "  tmp_x = -overlap_border\n",
        "  tmp_y = -overlap_border\n",
        "  x_offsets = [tmp_x]\n",
        "  y_offsets = [tmp_y]\n",
        "  while (tmp_x+image_size[0]-overlap_border) < ar_image_size[0]:\n",
        "    tmp_x += image_size[0]-overlap_border\n",
        "    x_offsets.append(tmp_x)\n",
        "  while (tmp_y+image_size[1]-overlap_border) < ar_image_size[1]:\n",
        "    tmp_y += image_size[1]-overlap_border\n",
        "    y_offsets.append(tmp_y)\n",
        "  return x_offsets, y_offsets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformation and noise settings"
      ],
      "metadata": {
        "id": "ypjCmFvBcZB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vertical_symmetry(x_in):\n",
        "  [n, c, h, w] = x_in.shape\n",
        "  return jnp.concatenate([x_in[:, :, :h//2, :], jnp.flip(x_in[:, :, :h//2, :],-2)], -2)\n",
        "\n",
        "def horizontal_symmetry(x_in):\n",
        "  [n, c, h, w] = x_in.shape\n",
        "  return jnp.concatenate([x_in[:, :, :, :w//2], jnp.flip(x_in[:, :, :, :w//2],-1)], -1)\n",
        "\n",
        "def inject_noise(x_in,img_in,noise_alpha,noise_scale,noise_symmetry):\n",
        "\n",
        "  # load img\n",
        "  tmp = load_image(img_in,image_size)\n",
        "\n",
        "  # make noise\n",
        "  noise = jax.random.normal(rng.split(), [batch_size, 3, image_size[1], image_size[0]])\n",
        "\n",
        "  # add noise to img\n",
        "  noise = (noise*noise_scale)+tmp\n",
        "\n",
        "  # force noise to have symmetry\n",
        "  if noise_symmetry == \"vertical\":\n",
        "    noise = vertical_symmetry(noise)\n",
        "  \n",
        "  if noise_symmetry == \"horizontal\":\n",
        "    noise = horizontal_symmetry(noise)\n",
        "\n",
        "  # noise type\n",
        "  mask = load_image(noise_alpha,image_size)\n",
        "\n",
        "  # apply mask to noise\n",
        "  inject  = noise*(mask.add(1).div(2))\n",
        "\n",
        "  # apply inverse mask to x\n",
        "  x_in = x_in*(mask.mul(-1).add(1).div(2))\n",
        "\n",
        "  return x_in + inject\n",
        "\n",
        "\n",
        "def x_transformation(x,t):\n",
        "  if use_horizontal_symmetry:\n",
        "    if t in t_schedule:\n",
        "      x = horizontal_symmetry(x)\n",
        "      print(\" horizontal symmetry applied\")\n",
        "  if use_vertical_symmetry:\n",
        "    if t in t_schedule:\n",
        "      x = vertical_symmetry(x)\n",
        "      print(\" vertical symmetry applied\")\n",
        "  if use_noise_inject:\n",
        "    if t in n_schedule:\n",
        "      n_index = n_schedule.index(t)\n",
        "      n_image = noise_inject_image_s[n_index]\n",
        "      n_alpha = noise_inject_alpha_s[n_index]\n",
        "      n_scale = noise_inject_scale[n_index]\n",
        "      n_symmetry = noise_inject_symmetry[n_index]\n",
        "      x = inject_noise(x,n_image,n_alpha,n_scale,n_symmetry)\n",
        "      print(\" noise inject\")\n",
        "  return x"
      ],
      "metadata": {
        "id": "Cl3DlB2qcXpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxtyHvbya7Ex"
      },
      "source": [
        "# Run Settings (Required)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wkkMfdl7-60L"
      },
      "outputs": [],
      "source": [
        "def req_params():\n",
        "\n",
        " #@markdown Run Settings (Required)\n",
        " #seed - used to determine \n",
        "  seed = 0 #@param\n",
        "  batch_size = 1 #@param {type:\"integer\"}\n",
        "  batch_folder = \"test\" #@param {type:\"string\"}\n",
        "  save_settings = True #@param {type:\"boolean\"}\n",
        "  #@markdown Diffusion Model Settings (Required)\n",
        "  steps =  100#@param {type:\"integer\"}\n",
        "  image_size = (256,256) #@param\n",
        "  diffusion_model = \"CC12M\" #@param [\"OpenAIFinetune\", \"CC12M\"]\n",
        "  use_secondary_model = False #@param {type:\"boolean\"}\n",
        "  sample_mode = 'ddim' #@param [\"ddim\", \"plms\", \"prk\"]\n",
        "  ddim_eta = 0.5 #@param\n",
        "  starting_noise =  1.0#@param\n",
        "  ending_noise = 0.0 #@param\n",
        "  #@markdown CLIP Model Settings (Required)\n",
        "  use_vitb32 = True #@param {type:\"boolean\"}\n",
        "  use_vitb16 = True #@param {type:\"boolean\"}\n",
        "  use_vitl14 = False #@param {type:\"boolean\"}\n",
        "  use_vitl14_336px = False #@param {type:\"boolean\"}\n",
        "  all_prompts = \"scifi abstract geometric pattern | white:-1 |  https://uploads6.wikiart.org/images/salvador-dali/the-persistence-of-memory-1931.jpg!Large.jpg:0.01\" #@param {type:\"string\"}\n",
        "  all_clip_guidance_scale = 80000 #@param {type:\"integer\"}\n",
        "  #@markdown Cut Settings (Required)\n",
        "  cutn = 4 #@param {type:\"integer\"}\n",
        "  cut_batches =  4#@param {type:\"integer\"}\n",
        "  cut_pow = 1.0 #@param\n",
        "  cut_p_grey = 0.2 #@param\n",
        "  cut_p_flip = 0.5 #@param\n",
        "  cut_p_mixgrey = None #@param\n",
        "  #@markdown Conditonal Settings (Required)\n",
        "  tv_scale = 100 #@param {type:\"integer\"}\n",
        "  range_scale = 100 #@param {type:\"integer\"}\n",
        "  mean_scale = 100 #@param {type:\"integer\"}\n",
        "  var_scale =   100#@param {type:\"integer\"}\n",
        "  horizontal_symmetry_scale = 0 #@param {type:\"integer\"}\n",
        "  vertical_symmetry_scale = 0 #@param {type:\"integer\"}\n",
        "  return locals()\n",
        "\n",
        "params = req_params()\n",
        "globals().update(params)\n",
        "\n",
        "# prevent errors\n",
        "try:\n",
        "  print(f\"use_init_img: {use_init_img}\")\n",
        "except NameError:\n",
        "  use_init_img = False\n",
        "  print(f\"use_init_img: {use_init_img}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_batch_runs: {use_batch_runs}\")\n",
        "except NameError:\n",
        "  use_batch_runs = False\n",
        "  print(f\"use_batch_runs: {use_batch_runs}\")\n",
        "\n",
        "try:\n",
        "  print(f\"init_mse_scale: {init_mse_scale}\")\n",
        "except NameError:\n",
        "  init_mse_scale = False\n",
        "  print(f\"init_mse_scale: {init_mse_scale}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_masked_mse: {use_masked_mse}\")\n",
        "except NameError:\n",
        "  use_masked_mse = False\n",
        "  print(f\"use_masked_mse: {use_masked_mse}\")\n",
        "  \n",
        "try:\n",
        "  print(f\"use_noise_inject: {use_noise_inject}\")\n",
        "except NameError:\n",
        "  use_noise_inject = False\n",
        "  print(f\"use_noise_inject: {use_noise_inject}\")\n",
        "\n",
        "try:\n",
        "  print(f\"save_batch_settings: {save_batch_settings}\")\n",
        "except NameError:\n",
        "  save_batch_settings = False\n",
        "  print(f\"save_batch_settings: {save_batch_settings}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_random_settings: {use_random_settings}\")\n",
        "except NameError:\n",
        "  use_random_settings = False\n",
        "  print(f\"use_random_settings: {use_random_settings}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_multiple_prompts: {use_multiple_prompts}\")\n",
        "except NameError:\n",
        "  use_multiple_prompts = False\n",
        "  print(f\"use_multiple_prompts: {use_multiple_prompts}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_vertical_symmetry: {use_vertical_symmetry}\")\n",
        "except NameError:\n",
        "  use_vertical_symmetry = False\n",
        "  print(f\"use_vertical_symmetry: {use_vertical_symmetry}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_automated_stitching: {use_automated_stitching}\")\n",
        "except NameError:\n",
        "  use_automated_stitching = False\n",
        "  print(f\"use_automated_stitching: {use_automated_stitching}\")\n",
        "\n",
        "try:\n",
        "  print(f\"use_horizontal_symmetry: {use_horizontal_symmetry}\")\n",
        "except NameError:\n",
        "  use_horizontal_symmetry = False\n",
        "  print(f\"use_horizontal_symmetry: {use_horizontal_symmetry}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Settings (Optional)"
      ],
      "metadata": {
        "id": "80tykxS_o0T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def opt_params():\n",
        "  #@markdown Multiple Prompt Settings (Optional)\n",
        "  use_multiple_prompts = False #@param {type:\"boolean\"}\n",
        "  vitb32_all_prompt = \"scifi abstract geometric pattern | white:-1\" #@param {type:\"string\"}\n",
        "  vitb32_clip_guidance_scale = 120000 #@param {type:\"integer\"}\n",
        "  vitb16_all_prompt = \"https://uploads6.wikiart.org/images/salvador-dali/the-persistence-of-memory-1931.jpg!Large.jpg\" #@param {type:\"string\"}\n",
        "  vitb16_clip_guidance_scale = 40000 #@param {type:\"integer\"}\n",
        "  vitl14_all_prompt = \"\" #@param {type:\"string\"}\n",
        "  vitl14_clip_guidance_scale = 0 #@param {type:\"integer\"}\n",
        "  vitl14_336px_all_prompt = \"\" #@param {type:\"string\"}\n",
        "  vitl14_336px_clip_guidance_scale = 0 #@param {type:\"integer\"}\n",
        "  #@markdown Initial Image Settings (Optional)\n",
        "  use_init_img = False #@param {type:\"boolean\"}\n",
        "  use_masked_mse = False #@param {type:\"boolean\"}\n",
        "  init_path = \"init01.png\" #@param {type:\"string\"}\n",
        "  init_mse_alpha_path = \"alpha01.png\" #@param {type:\"string\"}\n",
        "  init_append_path = link_path #@param\n",
        "  init_mse_scale = 1000 #@param {type:\"integer\"}\n",
        "  #@markdown Transformation Settings (Optional)\n",
        "  use_vertical_symmetry = False #@param {type:\"boolean\"}\n",
        "  use_horizontal_symmetry = False #@param {type:\"boolean\"}\n",
        "  transformation_schedule = [0.1,0.2,0.3] #@param\n",
        "  #@markdown Noise Inject (Optional)\n",
        "  use_noise_inject = False #@param {type:\"boolean\"}\n",
        "  noise_inject_alpha = [\"alpha01.png\"] #@param\n",
        "  noise_inject_image = [\"inject01.png\"] #@param\n",
        "  noise_inject_scale = [1] #@param\n",
        "  noise_inject_symmetry = [\"vertical\"] #@param\n",
        "  noise_inject_schedule = [0.2] #@param\n",
        "  noise_inject_append_path = link_path #@param\n",
        "  #@markdown Automated Stitching Settings (Optional)\n",
        "  use_automated_stitching = False #@param {type:\"boolean\"}\n",
        "  as_image_size = (512,512) #@param\n",
        "  as_starting_noise = 0.70 #@param\n",
        "  as_image_weight = 0.80 #@param\n",
        "  as_alpha_border = 40 #@param\n",
        "  as_alpha_feather =  20#@param\n",
        "  as_n_pass =  3#@param\n",
        "  as_stitch_overlap = 128 #@param\n",
        "  as_stitch_shift = [0,128,-128] #@param\n",
        "  #@markdown Automated Stitching Initial Image Settings (Optional)\n",
        "  use_as_init_img = False #@param {type:\"boolean\"}\n",
        "  as_init_path = \"init01.png\" #@param {type:\"string\"}\n",
        "  as_init_append_path = link_path #@param\n",
        "  as_init_mse_scale = 0 #@param\n",
        "  if (use_automated_stitching and use_as_init_img):\n",
        "    use_init_img = False\n",
        "    init_path = \"\"\n",
        "    init_mse_scale = as_init_mse_scale\n",
        "  if use_automated_stitching:\n",
        "    starting_noise = as_starting_noise\n",
        "  return locals()\n",
        "\n",
        "tmp_params = opt_params()\n",
        "params.update(tmp_params)\n",
        "globals().update(tmp_params)"
      ],
      "metadata": {
        "id": "Md1J_tT-oN1m",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZQrrKrKDdoHE"
      },
      "outputs": [],
      "source": [
        "#@markdown Test Automated Stitching\n",
        "tmp_path = \"/content/tmp/\"\n",
        "!mkdir -p $tmp_path\n",
        "\n",
        "as_img = -1*jnp.ones((1,3,as_image_size[1],as_image_size[0]))\n",
        "as_alpha = create_as_alpha(image_size,as_alpha_border,as_alpha_feather)\n",
        "x_schedule, y_schedule = create_as_offsets(image_size,as_image_size,as_stitch_overlap)\n",
        "\n",
        "as_n_frame = 0\n",
        "as_max_frame = as_n_pass*len(x_schedule)*len(y_schedule)\n",
        "\n",
        "for nn in range(as_n_pass):\n",
        "  for xx in x_schedule:\n",
        "    for yy in y_schedule:\n",
        "\n",
        "        if nn > (len(as_stitch_shift)-1):\n",
        "          shift = np.random.choice(as_stitch_shift,1)[0]\n",
        "        else:\n",
        "          shift = as_stitch_shift[nn]\n",
        "\n",
        "        #print(xx,yy)\n",
        "        as_cuts = as_cutout_image(as_img, xx+shift, yy+shift, image_size)\n",
        "\n",
        "        w1 = xx+shift\n",
        "        w2 = xx+shift+image_size[0]\n",
        "        h1 = yy+shift\n",
        "        h2 = yy+shift+image_size[1]\n",
        "\n",
        "        w1_diff = 0\n",
        "        w2_diff = image_size[0]\n",
        "        h1_diff = 0\n",
        "        h2_diff = image_size[1]\n",
        "\n",
        "        if w1 < 0:\n",
        "          w1_diff = 0 - w1\n",
        "        if w2 > as_image_size[0]:\n",
        "          w2_diff = image_size[0]-(w2-as_image_size[0])\n",
        "\n",
        "        if h1 < 0:\n",
        "          h1_diff = 0 - h1\n",
        "        if h2 > as_image_size[1]:\n",
        "          h2_diff = image_size[1]-(h2-as_image_size[1])\n",
        "\n",
        "        as_pred = jnp.ones((1,3,image_size[1],image_size[0]))\n",
        "        as_pred = ((as_cuts.add(1).div(2)*(1-as_alpha.add(1).div(2)))+(as_pred.add(1).div(2)*as_alpha.add(1).div(2))).mul(2).sub(1)\n",
        "        as_pred = as_cuts*(1-as_image_weight)+as_pred*as_image_weight\n",
        "        as_img = as_img.at[:,:,h1+h1_diff:h2+(image_size[1]-h2_diff),w1+w1_diff:w2+(image_size[0]-w2_diff)].set(as_pred[:,:,h1_diff:h2_diff,w1_diff:w2_diff])\n",
        "        \n",
        "        #print(\"cut\")\n",
        "        #display_images(as_cuts)\n",
        "        #print(\"pred\")\n",
        "        #display_images(as_pred)\n",
        "        #display_images(ar_img)\n",
        "\n",
        "        images = as_img.add(1).div(2).clamp(0, 1)\n",
        "        images = torch.tensor(np.array(images))\n",
        "        pil_image = TF.to_pil_image(images[0])\n",
        "        pil_image.save(f'{tmp_path}test_{str(as_n_frame).zfill(len(str(as_max_frame)))}.png')\n",
        "        as_n_frame += 1\n",
        "\n",
        "import glob\n",
        "import subprocess\n",
        "from base64 import b64encode\n",
        "\n",
        "init_frame = 0\n",
        "last_frame = as_max_frame\n",
        "fps = 6\n",
        "image_path = f\"{tmp_path}test_%0{len(str(as_max_frame))}d.png\"\n",
        "filepath = f\"{tmp_path}test.mp4\"\n",
        "\n",
        "cmd = [\n",
        "      'ffmpeg',\n",
        "      '-y',\n",
        "      '-vcodec',\n",
        "      'png',\n",
        "      '-r',\n",
        "      str(fps),\n",
        "      '-start_number',\n",
        "      str(init_frame),\n",
        "      '-i',\n",
        "      image_path,\n",
        "      '-frames:v',\n",
        "      str(last_frame),\n",
        "      '-c:v',\n",
        "      'libx264',\n",
        "      '-vf',\n",
        "      f'fps={fps}',\n",
        "      '-pix_fmt',\n",
        "      'yuv420p',\n",
        "      '-crf',\n",
        "      '17',\n",
        "      '-preset',\n",
        "      'veryslow',\n",
        "      filepath\n",
        "      ]\n",
        "\n",
        "process = subprocess.Popen(cmd, cwd=f'{tmp_path}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "  print(stderr)\n",
        "  raise RuntimeError(stderr)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "mp4 = open(filepath,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display.HTML(\"\"\"\n",
        "      <video width=400 controls>\n",
        "            <source src=\"%s\" type=\"video/mp4\">\n",
        "      </video>\n",
        "      \"\"\" % data_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxWpKOkGRoZ5"
      },
      "source": [
        "# Batch Settings (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Nq7aCWcmOkG0"
      },
      "outputs": [],
      "source": [
        "def batch_params():\n",
        "\n",
        "  #@markdown Batch Run Settings (Optional)\n",
        "  use_batch_runs = False #@param {type:\"boolean\"}\n",
        "  n_batch_runs = 2 #@param {type:\"integer\"}\n",
        "  \n",
        "  #@markdown Batch Random Settings (Optional)\n",
        "  use_random_settings = False #@param {type:\"boolean\"}\n",
        "\n",
        "  #@markdown Batch Initial Image Settings (Optional)\n",
        "  use_random_init = False #@param {type:\"boolean\"}\n",
        "  use_ordered_init = False #@param {type:\"boolean\"}\n",
        "  batch_init_path = \"\" #@param {type:\"string\"}\n",
        "  batch_init_append_path = link_path #@param\n",
        "\n",
        "  #@markdown Batch Random Prompts (Optional)\n",
        "  use_random_prompts = False #@param {type:\"boolean\"}\n",
        "  prompts_path = \"prompts.csv\" #@param {type:\"string\"}\n",
        "  prompt_append_path = link_path #@param\n",
        "\n",
        "  return locals()\n",
        "\n",
        "tmp_params = batch_params()\n",
        "params.update(tmp_params)\n",
        "globals().update(params)\n",
        "\n",
        "# batch functions\n",
        "if use_batch_runs:\n",
        "  batch_all_prompts = [all_prompts]*n_batch_runs\n",
        "\n",
        "def random_settings():\n",
        "  \n",
        "  def rand_func(minval,maxval):\n",
        "    randval = np.random.randint(minval,maxval)\n",
        "    return(randval)\n",
        "\n",
        "  steps = rand_func(100,200)\n",
        "  eta = rand_func(-5,10)/10.0\n",
        "  clip_guidance_scale = rand_func(1000,100000)\n",
        "  tv_scale = rand_func(0,10000)\n",
        "  range_scale = rand_func(0,1000)\n",
        "  mean_scale = rand_func(-1000,1000)\n",
        "  var_scale = rand_func(-1000,1000)\n",
        "  horizontal_symmetry_scale = rand_func(-10000,10000)\n",
        "  vertical_symmetry_scale = rand_func(-10000,10000)\n",
        "\n",
        "  globals().update(locals())\n",
        "  return\n",
        "\n",
        "def get_random_prompts(n_batch_runs):\n",
        "  import pandas as pd\n",
        "  prompts = pd.read_csv(prompt_append_path+prompts_path)\n",
        "  batch_all_prompts = []\n",
        "  for ii in range(n_batch_runs):\n",
        "    temp = prompts.prompt.sample(1).item()\n",
        "    random_prompt_template = f\"{temp}\" #@param\n",
        "    batch_all_prompts.append(random_prompt_template)\n",
        "  return batch_all_prompts\n",
        "\n",
        "if use_random_prompts:\n",
        "  batch_all_prompts = get_random_prompts(n_batch_runs)\n",
        "\n",
        "def get_batch_prompts(n_batch_runs):\n",
        "  import pandas as pd\n",
        "\n",
        "  if (use_random_init) or (use_ordered_init):\n",
        "    try:\n",
        "      ext = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"]\n",
        "      filenames = []\n",
        "      for filename in os.listdir(batch_init_append_path+batch_init_path):\n",
        "        if os.path.splitext(filename)[1].lower() in ext:\n",
        "          filenames.append(filename)\n",
        "      file_df = pd.DataFrame({\"init_filenames\":sorted(filenames)})\n",
        "      #print(file_df)\n",
        "    except:\n",
        "      print(\"...path error\")\n",
        "\n",
        "  if use_random_init:\n",
        "    batch_init_paths = []\n",
        "    for i in range(n_batch_runs):\n",
        "      batch_init_paths.append(batch_init_append_path+batch_init_path+file_df.sample(1).values[0][0])\n",
        "\n",
        "  if use_ordered_init:\n",
        "    batch_init_paths = []\n",
        "    n_batch_runs = len(file_df)\n",
        "    for i in range(n_batch_runs):\n",
        "      batch_init_paths.append(batch_init_append_path+batch_init_path+file_df.loc[i].values[0])\n",
        "\n",
        "  return batch_init_paths, n_batch_runs\n",
        "\n",
        "if (use_random_init) or (use_ordered_init):\n",
        "  batch_init_paths, n_batch_runs = get_batch_prompts(n_batch_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2V_6VwChevf"
      },
      "outputs": [],
      "source": [
        "if use_batch_runs:\n",
        "  batch_all_prompts = [\"scifi abstract geometric pattern | white:-1 | https://uploads6.wikiart.org/images/salvador-dali/the-persistence-of-memory-1931.jpg!Large.jpg:0.01\",\n",
        "                       \"scifi abstract geometric pattern | white:-1 | blue:-1 | https://uploads6.wikiart.org/images/salvador-dali/the-persistence-of-memory-1931.jpg!Large.jpg:0.01\",\n",
        "                       \"scifi abstract geometric pattern | white:-1 | red:-1 | https://uploads6.wikiart.org/images/salvador-dali/the-persistence-of-memory-1931.jpg!Large.jpg:0.01\",\n",
        "                       \"scifi abstract geometric pattern | white:-1 | colorful:-1 | https://uploads6.wikiart.org/images/salvador-dali/the-persistence-of-memory-1931.jpg!Large.jpg:0.01\"\n",
        "                      ]\n",
        "\n",
        "  n_batch_runs = len(batch_all_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "zIQISXP1R2bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHrU5fZi7Qun",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from torch._C import parse_schema\n",
        "#@markdown Display Init\n",
        "display_init = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Display Rate\n",
        "save_rates = False #@param {type:\"boolean\"}\n",
        "display_rates = False #@param {type:\"boolean\"}\n",
        "rate = 50 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Display Percent\n",
        "save_percents = False #@param {type:\"boolean\"}\n",
        "display_percents = False #@param {type:\"boolean\"}\n",
        "percent_schedule = [0.4,0.6,0.8] #@param\n",
        "\n",
        "#@markdown Automated Stitching\n",
        "resume_automated_stitching = False #@param {type:\"boolean\"}\n",
        "save_intermediate_steps = False #@param {type:\"boolean\"}\n",
        "use_display_as_cuts = False #@param {type:\"boolean\"}\n",
        "use_display_as_pred = False #@param {type:\"boolean\"}\n",
        "use_display_as_img = False #@param {type:\"boolean\"}\n",
        "if resume_automated_stitching:\n",
        "  print(\"...resuming\")\n",
        "\n",
        "def config():\n",
        "\n",
        "  # clip\n",
        "  vitb16 = lambda: get_clip('ViT-B/16')\n",
        "  vitb32 = lambda: get_clip('ViT-B/32')\n",
        "  vitl14 = lambda: get_clip('ViT-L/14')\n",
        "  vitl14_336px = lambda: get_clip('ViT-L/14@336px')\n",
        "\n",
        "  # diffusion\n",
        "  if diffusion_model == 'OpenAIFinetune':\n",
        "    diffusion = openai_512_finetune()\n",
        "  if diffusion_model == 'CC12M':\n",
        "    diffusion = cc12m_1_wrap(cc12m_1_params(), clip_embed=process_prompts(vitb16(), title))\n",
        "  if diffusion_model == 'CC12M_Danbooru':\n",
        "    diffusion = cc12m_1_wrap(cc12m_danbooru_params(), clip_embed=process_prompts(vitb16(), title))\n",
        "\n",
        "  # secondary\n",
        "  if use_secondary_model:\n",
        "    cond_model = secondary2_wrap()\n",
        "  else:\n",
        "    cond_model = diffusion\n",
        "\n",
        "  # cuts\n",
        "  if use_vitb16 or use_vitb32 or use_vitl14:\n",
        "    make_cutouts = MakeCutouts(224, cutn, cut_pow=cut_pow, p_grey=cut_p_grey, p_flip=cut_p_flip, p_mixgrey=cut_p_mixgrey)\n",
        "  else:\n",
        "    make_cutouts = None\n",
        "  if use_vitl14_336px:\n",
        "    make_cutouts_336px = MakeCutouts(336, cutn, cut_pow=cut_pow, p_grey=cut_p_grey, p_flip=cut_p_flip, p_mixgrey=cut_p_mixgrey)\n",
        "  else:\n",
        "    make_cutouts_336px = None\n",
        "\n",
        "  # cond\n",
        "  cond_fn = MainCondFn(cond_model,[\n",
        "    CondCLIP(vitb32(), \n",
        "            make_cutouts, \n",
        "            cut_batches, SphericalDistLoss(process_prompts(vitb32(), vitb32_prompt), vitb32_cgs)) if use_vitb32 and vitb32_cgs > 0 else None,                               \n",
        "    CondCLIP(vitb16(), \n",
        "            make_cutouts, \n",
        "            cut_batches, SphericalDistLoss(process_prompts(vitb16(), vitb16_prompt), vitb16_cgs)) if use_vitb16 and vitb16_cgs > 0 else None,\n",
        "    CondCLIP(vitl14(), \n",
        "            make_cutouts, \n",
        "            cut_batches, SphericalDistLoss(process_prompts(vitl14(), vitl14_prompt), vitl14_cgs)) if use_vitl14 and vitl14_cgs > 0 else None,\n",
        "    CondCLIP(vitl14_336px(), \n",
        "            make_cutouts_336px, \n",
        "            cut_batches, SphericalDistLoss(process_prompts(vitl14_336px(), vitl14_336px_prompt), vitl14_336px_cgs)) if use_vitl14_336px and vitl14_336px_cgs > 0 else None,\n",
        "    CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "    CondRange(range_scale) if range_scale > 0 else None,\n",
        "    CondMean(mean_scale) if mean_scale > 0 else None,\n",
        "    CondVar(var_scale) if var_scale > 0 else None,\n",
        "    CondHorizontalSymmetry(horizontal_symmetry_scale) if horizontal_symmetry_scale > 0 else None,\n",
        "    CondVerticalSymmetry(vertical_symmetry_scale) if vertical_symmetry_scale > 0 else None,\n",
        "    CondMSE(init_array, init_mse_scale) if init_mse_scale > 0 and ((use_init_img or use_as_init_img) and not use_masked_mse) else None,\n",
        "    CondMaskedMSE(init_array, init_mse_scale, init_mse_alpha) if (init_mse_scale > 0 and ((use_init_img or use_as_init_img) and use_masked_mse)) else None\n",
        "  ])\n",
        "    \n",
        "  return diffusion, cond_fn\n",
        "\n",
        "@torch.no_grad()\n",
        "def run():\n",
        "\n",
        "  global rng\n",
        "\n",
        "  alphas, sigmas = cosine.to_alpha_sigma(schedule)\n",
        "\n",
        "  x = jax.random.normal(rng.split(), [batch_size, 3, image_size[1], image_size[0]])\n",
        "\n",
        "  if init_array is not None:\n",
        "    x = sigmas[0] * x + alphas[0] * init_array\n",
        "\n",
        "  # main loop\n",
        "  if sample_mode == 'ddim':\n",
        "    sample_loop = partial(sampler.ddim_sample_loop, eta=ddim_eta)\n",
        "  if sample_mode == 'prk':\n",
        "    sample_loop = sampler.prk_sample_loop\n",
        "  if sample_mode == 'plms':\n",
        "    sample_loop = sampler.plms_sample_loop\n",
        "    \n",
        "  for output in sample_loop(diffusion, cond_fn, x, schedule, rng.split(), x_fn = x_transformation):\n",
        "    j = output['step']\n",
        "    pred = output['pred']\n",
        "    assert x.isfinite().all().item()\n",
        "\n",
        "    # rate\n",
        "    if ((j % rate) == 0 and display_rates) and (j not in [0,len(schedule)-1]):\n",
        "      display_images(pred)\n",
        "    \n",
        "    if ((j % rate) == 0 and save_rates) and (j not in [0,len(schedule)-1]):\n",
        "      images = pred.add(1).div(2).clamp(0, 1)\n",
        "      images = torch.tensor(np.array(images))\n",
        "      for k in range(batch_size):\n",
        "        pil_image = TF.to_pil_image(images[k])\n",
        "        pil_image.save(f'{out_path}{timestring}_{k}_{j}.png')\n",
        "\n",
        "    # percent\n",
        "    if ((j in display_steps) and display_percents) and j != len(schedule)-1:\n",
        "      display_images(pred)\n",
        "\n",
        "    if ((j in display_steps) and save_percents) and j != len(schedule)-1:\n",
        "      images = pred.add(1).div(2).clamp(0, 1)\n",
        "      images = torch.tensor(np.array(images))\n",
        "      for k in range(batch_size):\n",
        "        pil_image = TF.to_pil_image(images[k])\n",
        "        pil_image.save(f'{out_path}{timestring}_{k}_{j}.png')\n",
        "           \n",
        "    # save samples\n",
        "    # display_images(output['pred'])\n",
        "\n",
        "  return(output['pred'])\n",
        "\n",
        "# main\n",
        "try:\n",
        "\n",
        "  if use_batch_runs:\n",
        "    pass\n",
        "  else:\n",
        "    batch_all_prompts = [0]\n",
        "\n",
        "  for ii in range(len(batch_all_prompts)):\n",
        "\n",
        "    if use_batch_runs:\n",
        "      print(f\"...starting run {ii+1} of {len(batch_all_prompts)}\")\n",
        "      all_prompts = batch_all_prompts[ii]\n",
        "    else:\n",
        "      print(f\"...starting run\")\n",
        "\n",
        "    if use_multiple_prompts:\n",
        "      print(f\"...using multiple prompts\")\n",
        "    else:\n",
        "      print(f\"...{strip_weights_from_prompt(all_prompts)}\")\n",
        "\n",
        "    if use_random_settings:\n",
        "      random_settings()\n",
        "      print(f\"...using random settings\")\n",
        "\n",
        "    # seed\n",
        "    if seed > 0:\n",
        "      tmp_seed = seed\n",
        "      rng = PRNG(jax.random.PRNGKey(seed))\n",
        "    else:\n",
        "      local_seed = int(time.time())\n",
        "      params[\"local_seed\"] = local_seed\n",
        "      rng = PRNG(jax.random.PRNGKey(local_seed))\n",
        "\n",
        "    # timestring\n",
        "    timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "    \n",
        "    # display steps\n",
        "    if display_percents or save_percents:\n",
        "      display_steps = [int(steps*percent) for percent in percent_schedule]\n",
        "    else:\n",
        "      display_steps = []\n",
        "\n",
        "    # init\n",
        "    if use_automated_stitching and resume_automated_stitching:\n",
        "      xx_start, yy_start, as_n_start = ix, iy, nn\n",
        "    elif use_automated_stitching:\n",
        "      if use_as_init_img:\n",
        "        as_img = load_image(as_init_append_path+as_init_path,as_image_size)\n",
        "      else:\n",
        "        as_img = jax.random.normal(rng.split(), [batch_size, 3, as_image_size[1], as_image_size[0]])\n",
        "      as_mask = jnp.ones((1,3,as_image_size[1],as_image_size[0])).clamp(0,1)\n",
        "      as_alpha = create_as_alpha(image_size,as_alpha_border,as_alpha_feather)\n",
        "      x_schedule, y_schedule = create_as_offsets(image_size,as_image_size,as_stitch_overlap)\n",
        "      xx_start, yy_start, as_n_start = 0, 0, 0\n",
        "    elif use_batch_runs and (use_random_init or use_ordered_init):\n",
        "      init_array = load_image(batch_init_paths[ii],image_size)\n",
        "      as_n_pass, x_schedule, y_schedule = 1, [1], [1]\n",
        "      xx_start, yy_start, as_n_start = 0, 0, 0\n",
        "    elif use_init_img:\n",
        "      try:\n",
        "        init_array = load_image(init_append_path+init_path,image_size)\n",
        "      except:\n",
        "        init_array = None\n",
        "        use_init_img = False\n",
        "      try:\n",
        "        init_mse_alpha = load_image(init_append_path+init_mse_alpha_path,image_size)\n",
        "      except:\n",
        "        init_mse_alpha = None\n",
        "      as_n_pass, x_schedule, y_schedule = 1, [1], [1]\n",
        "      xx_start, yy_start, as_n_start = 0, 0, 0\n",
        "    elif use_masked_mse:\n",
        "      init_array = load_image(init_append_path+init_path,image_size)\n",
        "      init_mse_alpha = load_image(init_append_path+init_mse_alpha_path,image_size)\n",
        "      as_n_pass, x_schedule, y_schedule = 1, [1], [1]\n",
        "      xx_start, yy_start, as_n_start = 0, 0, 0\n",
        "    else:\n",
        "      init_array = None\n",
        "      as_n_pass, x_schedule, y_schedule = 1, [1], [1]\n",
        "      xx_start, yy_start, as_n_start = 0, 0, 0\n",
        "\n",
        "    for nn in range(as_n_start,as_n_pass):\n",
        "      if use_automated_stitching:\n",
        "        print(f\"...starting pass {nn+1} of {(len(x_schedule)-xx_start)*(len(y_schedule)-yy_start)}\")\n",
        "      for ix in range(xx_start,len(x_schedule)):\n",
        "        for iy in range(yy_start,len(y_schedule)):\n",
        "            \n",
        "            # clip\n",
        "            title = expand([all_prompts], batch_size)\n",
        "            vitb32_cgs = all_clip_guidance_scale\n",
        "            vitb32_prompt = title\n",
        "            vitb16_cgs = all_clip_guidance_scale\n",
        "            vitb16_prompt = title\n",
        "            vitl14_cgs = all_clip_guidance_scale\n",
        "            vitl14_prompt = title\n",
        "            vitl14_336px_cgs = all_clip_guidance_scale\n",
        "            vitl14_336px_prompt = title\n",
        "\n",
        "            if use_multiple_prompts:\n",
        "              vitb32_cgs = vitb32_clip_guidance_scale\n",
        "              vitb32_prompt = expand([vitb32_all_prompt], batch_size)\n",
        "              vitb16_cgs = vitb16_clip_guidance_scale\n",
        "              vitb16_prompt = expand([vitb16_all_prompt], batch_size)\n",
        "              vitl14_cgs = vitl14_clip_guidance_scale\n",
        "              vitl14_prompt = expand([vitl14_all_prompt], batch_size)\n",
        "              vitl14_336px_cgs = vitl14_336px_clip_guidance_scale\n",
        "              vitl14_336px_prompt = expand([vitl14_336px_all_prompt], batch_size)\n",
        "            \n",
        "            schedule = jnp.linspace(starting_noise, ending_noise, steps+1)\n",
        "            schedule = spliced.to_cosine(schedule)\n",
        "            out_path = get_output_folder(output_path, batch_folder)\n",
        "            diffusion, cond_fn = config()\n",
        "\n",
        "            # transformation functions\n",
        "            if use_vertical_symmetry or use_horizontal_symmetry:\n",
        "              transformation_steps = [int(steps*i) for i in transformation_schedule]\n",
        "              t_schedule = [schedule[i] for i in transformation_steps]\n",
        "\n",
        "            # noise injects\n",
        "            if use_noise_inject:\n",
        "              noise_inject_steps = [int(steps*i) for i in noise_inject_schedule]\n",
        "              n_schedule = [schedule[i] for i in noise_inject_steps]\n",
        "              noise_inject_image_s = [noise_inject_append_path+i for i in noise_inject_image]\n",
        "              noise_inject_alpha_s = [noise_inject_append_path+i for i in noise_inject_alpha]\n",
        "\n",
        "            if use_automated_stitching:\n",
        "\n",
        "              # timer\n",
        "              tic = time.perf_counter()\n",
        "\n",
        "              xx = x_schedule[ix]\n",
        "              yy = y_schedule[iy]\n",
        "\n",
        "              if nn > (len(as_stitch_shift)-1):\n",
        "                shift = np.random.choice(as_stitch_shift,1)[0]\n",
        "              else:\n",
        "                shift = as_stitch_shift[nn]\n",
        "\n",
        "              #print(xx,yy)\n",
        "              as_cuts = as_cutout_image(as_img, xx+shift, yy+shift, image_size)\n",
        "              #as_mask = as_cutout_image(as_light, xx+shift, yy+shift, image_size)\n",
        "              init_array = as_cuts\n",
        "              \n",
        "              w1 = xx+shift\n",
        "              w2 = xx+shift+image_size[0]\n",
        "              h1 = yy+shift\n",
        "              h2 = yy+shift+image_size[1]\n",
        "\n",
        "              w1_diff = 0\n",
        "              w2_diff = image_size[0]\n",
        "              h1_diff = 0\n",
        "              h2_diff = image_size[1]\n",
        "\n",
        "              if w1 < 0:\n",
        "                w1_diff = 0 - w1\n",
        "              if w2 > as_image_size[0]:\n",
        "                w2_diff = image_size[0]-(w2-as_image_size[0])\n",
        "\n",
        "              if h1 < 0:\n",
        "                h1_diff = 0 - h1\n",
        "              if h2 > as_image_size[1]:\n",
        "                h2_diff = image_size[1]-(h2-as_image_size[1])\n",
        "\n",
        "              as_pred = run()\n",
        "\n",
        "              if save_intermediate_steps:\n",
        "                #display_images(as_pred)\n",
        "                images = as_pred.add(1).div(2).clamp(0, 1)\n",
        "                images = torch.tensor(np.array(images))\n",
        "                pil_image = TF.to_pil_image(images[0])\n",
        "                pil_image.save(f'{out_path}{timestring}_{ix}_{iy}.png')\n",
        "\n",
        "              as_pred = ((as_cuts.add(1).div(2)*(1-as_alpha.add(1).div(2)))+(as_pred.add(1).div(2)*as_alpha.add(1).div(2))).mul(2).sub(1)\n",
        "              as_pred = as_cuts*(1-as_image_weight)+as_pred*as_image_weight\n",
        "              as_img = as_img.at[:,:,h1+h1_diff:h2+(image_size[1]-h2_diff),w1+w1_diff:w2+(image_size[0]-w2_diff)].set(as_pred[:,:,h1_diff:h2_diff,w1_diff:w2_diff])\n",
        "\n",
        "              if use_display_as_cuts:\n",
        "                display_images(as_cuts)\n",
        "\n",
        "              if use_display_as_pred:\n",
        "                display_images(as_pred)\n",
        "              \n",
        "              if use_display_as_img:\n",
        "                display_images(as_img)\n",
        "              \n",
        "              #display_images(as_mask)\n",
        "\n",
        "              images = as_img.add(1).div(2).clamp(0, 1)\n",
        "              images = torch.tensor(np.array(images))\n",
        "              pil_image = TF.to_pil_image(images[0])\n",
        "              pil_image.save(f'{out_path}{timestring}.png')\n",
        "\n",
        "              # timer\n",
        "              toc = time.perf_counter()\n",
        "              print_time_remaining()\n",
        "\n",
        "            else:\n",
        "\n",
        "              pred = run()\n",
        "              \n",
        "              # save samples\n",
        "              display_images(pred)\n",
        "              images = pred.add(1).div(2).clamp(0, 1)\n",
        "              images = torch.tensor(np.array(images))\n",
        "              for k in range(batch_size):\n",
        "                pil_image = TF.to_pil_image(images[k])\n",
        "                pil_image.save(f'{out_path}{timestring}_{k}.png')\n",
        "\n",
        "      if use_automated_stitching:\n",
        "        display_images(as_img)\n",
        "    \n",
        "    if save_settings:\n",
        "      with open(f\"{out_path}{timestring}_settings.txt\", \"w+\") as f:\n",
        "        json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "      \n",
        "    success = True\n",
        "\n",
        "except:\n",
        "  import traceback\n",
        "  traceback.print_exc()\n",
        "  success = False\n",
        "assert success"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QJ5aW34zTJBX"
      ],
      "name": "Huemin Jax Diffusion 2.7 ",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}